{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/csuarezgurruchaga/Desktop/Digital-House/CLASE_38/dsad_2021/common\n",
      "default checking\n",
      "Running command `conda list`... ok\n",
      "jupyterlab=2.2.6 already installed\n",
      "pandas=1.1.5 already installed\n",
      "bokeh=2.2.3 already installed\n",
      "seaborn=0.11.0 already installed\n",
      "matplotlib=3.3.2 already installed\n",
      "ipywidgets=7.5.1 already installed\n",
      "pytest=6.2.1 already installed\n",
      "chardet=4.0.0 already installed\n",
      "psutil=5.7.2 already installed\n",
      "scipy=1.5.2 already installed\n",
      "statsmodels=0.12.1 already installed\n",
      "scikit-learn=0.23.2 already installed\n",
      "xlrd=2.0.1 already installed\n",
      "Running command `conda install --yes nltk=3.5.0`... ok\n",
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "unidecode=1.1.1 already installed\n",
      "pydotplus=2.0.2 already installed\n",
      "pandas-datareader=0.9.0 already installed\n",
      "flask=1.1.2 already installed\n"
     ]
    }
   ],
   "source": [
    "# initial setup\n",
    "%run \"../../../common/0_notebooks_base_setup.py\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<link rel=\"stylesheet\" href=\"../../../common/dhds.css\">\n",
    "<div class=\"Table\">\n",
    "    <div class=\"Row\">\n",
    "        <div class=\"Cell grey left\"> <img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_38_Boosting/M5_CLASE_38_portada.png\" align=\"center\" width=\"70%\"/></div>\n",
    "        <div class=\"Cell right\">\n",
    "            <div class=\"div-logo\"><img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/common/logo_DH.png\" align=\"center\" width=70% /></div>\n",
    "            <div class=\"div-curso\">DATA SCIENCE</div>\n",
    "            <div class=\"div-modulo\">MÓDULO 5</div>\n",
    "            <div class=\"div-contenido\">Boosting <br/> Gradient Boosting <br/> XGBoost</div>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Agenda\n",
    "\n",
    "---\n",
    "\n",
    "- Boosting\n",
    "\n",
    "- Gradient Boosting\n",
    "\n",
    "- XGBoost\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Boosting\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_38_Boosting/M5_CLASE_38_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introducción\n",
    "\n",
    "---\n",
    "\n",
    "<table><tr><td style=\"font-size:16px;width:55%;line-height:2;\"><ul> \n",
    "<li>Con <b>Bagging y Random Forests</b> entrenamos modelos en subsets separados y luego combinamos su predicción. Estamos paralelizando el entrenamiento y luego combinando los resultados.</li>\n",
    "\n",
    "<li> <b>Boosting</b> es otra técnica de ensamble en la que el entrenamiento es <b>secuencial</b>.</li>\n",
    "</ul>    \n",
    "</td>    \n",
    "<td>\n",
    "<img src = \"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_38_Boosting/M5_CLASE_38_boosting_2.png\"  />\n",
    "</td>        \n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<table><tr><td style=\"font-size:16px;width:55%;line-height:2;\"><ul> \n",
    "<li>\n",
    "<b>Boosting</b> es un <b>procedimiento iterativo</b> que va construyendo un modelo final en pasos.\n",
    "</li>\n",
    "\n",
    "<li>\n",
    "En cada nuevo paso intentará aprender de los errores cometidos en los pasos previos. Se entrena una secuencia de modelos donde se da más peso a los ejemplos que fueron clasificados erróneamente por iteraciones anteriores. \n",
    "</li>\n",
    "\n",
    "<li>    \n",
    "<b>Trabaja sobre los errores del modelo anterior</b> o bien usándolos para cambiar la ponderación en el siguiente modelo o bien entrenando un modelo que prediga los mismos.\n",
    "</li>\n",
    "\n",
    "<li>Al igual que con bagging, las tareas de clasificación se resuelven con una mayoría ponderada de votos, y las tareas de regresión se resuelven con una suma ponderada para producir la predicción final.</li>\n",
    "    \n",
    "</ul>    \n",
    "</td>    \n",
    "<td>    \n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_38_Boosting/M5_CLASE_38_bagging_vs_boosting_1.png\" align=\"center\"/>\n",
    "</td>        \n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Adaboost\n",
    "\n",
    "---\n",
    "\n",
    "<table><tr><td style=\"font-size:16px;width:55%;line-height:2;\"><ul> \n",
    "<li>\n",
    "La primera iteración utiliza <b>pesos uniformes</b> para todos los registros. En las iteraciones posteriores, los pesos se ajustan para <b>enfatizar los errores</b> en la iteración anterior.\n",
    "</li>\n",
    "<li>\n",
    "La predicción final se construye mediante un <b>voto ponderado</b> de los distintos modelos base. Donde los pesos para cada modelo base dependen de su error de entrenamiento.\n",
    "</li>\n",
    "<li>\n",
    "Adaboost toma un <b>modelo base débil</b> e intenta hacerlo fuerte al reentrenarlo en las muestras mal clasificadas.\n",
    "</li>\n",
    "        \n",
    "</ul>    \n",
    "</td>    \n",
    "<td>    \n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_38_Boosting/M5_CLASE_38_boosting_1.png\" align=\"center\"/>\n",
    "</td>        \n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Algoritmo Adaboost\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_38_Boosting/M5_CLASE_38_adaboost_1.png\" align=\"center\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_38_Boosting/M5_CLASE_38_adaboost_2.png\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_38_Boosting/M5_CLASE_38_adaboost_3.png\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_38_Boosting/M5_CLASE_38_adaboost_4.png\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_38_Boosting/M5_CLASE_38_adaboost_5.png\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_38_Boosting/M5_CLASE_38_adaboost_6.png\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_38_Boosting/M5_CLASE_38_adaboost_7.png\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ejemplo\n",
    "\n",
    "---\n",
    "\n",
    "Vamos a usar el dataset Hitters (que ya usamos en CART, Ensambles y Bagging) para entrenar un modelo de regresión usando AdaBoostRegressor para predecir el valor del `log(Salary)`. \n",
    "\n",
    "El modelo base que usaremos en una regresión lineal.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Leemos los datos y, para simplificar, nos quedamos sólo con los registros completos y las features numéricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(322, 20)\n",
      "(263, 20)\n",
      "(263, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AtBat</th>\n",
       "      <th>Hits</th>\n",
       "      <th>HmRun</th>\n",
       "      <th>Runs</th>\n",
       "      <th>RBI</th>\n",
       "      <th>Walks</th>\n",
       "      <th>Years</th>\n",
       "      <th>CAtBat</th>\n",
       "      <th>CHits</th>\n",
       "      <th>CHmRun</th>\n",
       "      <th>CRuns</th>\n",
       "      <th>CRBI</th>\n",
       "      <th>CWalks</th>\n",
       "      <th>PutOuts</th>\n",
       "      <th>Assists</th>\n",
       "      <th>Errors</th>\n",
       "      <th>Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>315</td>\n",
       "      <td>81</td>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>38</td>\n",
       "      <td>39</td>\n",
       "      <td>14</td>\n",
       "      <td>3449</td>\n",
       "      <td>835</td>\n",
       "      <td>69</td>\n",
       "      <td>321</td>\n",
       "      <td>414</td>\n",
       "      <td>375</td>\n",
       "      <td>632</td>\n",
       "      <td>43</td>\n",
       "      <td>10</td>\n",
       "      <td>475.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>479</td>\n",
       "      <td>130</td>\n",
       "      <td>18</td>\n",
       "      <td>66</td>\n",
       "      <td>72</td>\n",
       "      <td>76</td>\n",
       "      <td>3</td>\n",
       "      <td>1624</td>\n",
       "      <td>457</td>\n",
       "      <td>63</td>\n",
       "      <td>224</td>\n",
       "      <td>266</td>\n",
       "      <td>263</td>\n",
       "      <td>880</td>\n",
       "      <td>82</td>\n",
       "      <td>14</td>\n",
       "      <td>480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>496</td>\n",
       "      <td>141</td>\n",
       "      <td>20</td>\n",
       "      <td>65</td>\n",
       "      <td>78</td>\n",
       "      <td>37</td>\n",
       "      <td>11</td>\n",
       "      <td>5628</td>\n",
       "      <td>1575</td>\n",
       "      <td>225</td>\n",
       "      <td>828</td>\n",
       "      <td>838</td>\n",
       "      <td>354</td>\n",
       "      <td>200</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>321</td>\n",
       "      <td>87</td>\n",
       "      <td>10</td>\n",
       "      <td>39</td>\n",
       "      <td>42</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>396</td>\n",
       "      <td>101</td>\n",
       "      <td>12</td>\n",
       "      <td>48</td>\n",
       "      <td>46</td>\n",
       "      <td>33</td>\n",
       "      <td>805</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>91.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>594</td>\n",
       "      <td>169</td>\n",
       "      <td>4</td>\n",
       "      <td>74</td>\n",
       "      <td>51</td>\n",
       "      <td>35</td>\n",
       "      <td>11</td>\n",
       "      <td>4408</td>\n",
       "      <td>1133</td>\n",
       "      <td>19</td>\n",
       "      <td>501</td>\n",
       "      <td>336</td>\n",
       "      <td>194</td>\n",
       "      <td>282</td>\n",
       "      <td>421</td>\n",
       "      <td>25</td>\n",
       "      <td>750.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AtBat  Hits  HmRun  Runs  RBI  Walks  Years  CAtBat  CHits  CHmRun  CRuns  \\\n",
       "1    315    81      7    24   38     39     14    3449    835      69    321   \n",
       "2    479   130     18    66   72     76      3    1624    457      63    224   \n",
       "3    496   141     20    65   78     37     11    5628   1575     225    828   \n",
       "4    321    87     10    39   42     30      2     396    101      12     48   \n",
       "5    594   169      4    74   51     35     11    4408   1133      19    501   \n",
       "\n",
       "   CRBI  CWalks  PutOuts  Assists  Errors  Salary  \n",
       "1   414     375      632       43      10   475.0  \n",
       "2   266     263      880       82      14   480.0  \n",
       "3   838     354      200       11       3   500.0  \n",
       "4    46      33      805       40       4    91.5  \n",
       "5   336     194      282      421      25   750.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw = pd.read_csv(\"../Data/Hitters.csv\")\n",
    "print(data_raw.shape)\n",
    "data_complete = data_raw.dropna()\n",
    "print(data_complete.shape)\n",
    "\n",
    "data_columns = ['AtBat', 'Hits', 'HmRun', 'Runs', 'RBI', \n",
    "                'Walks', 'Years', 'CAtBat', 'CHits', 'CHmRun', \n",
    "                'CRuns', 'CRBI', 'CWalks', 'PutOuts', 'Assists', \n",
    "                'Errors', 'Salary']\n",
    "\n",
    "data = data_complete.loc[:, data_columns]\n",
    "print(data.shape)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Creamos los conjuntos de train y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(263, 16)\n",
      "(263,)\n"
     ]
    }
   ],
   "source": [
    "X = data.drop(\"Salary\", axis = 1)\n",
    "print(X.shape)\n",
    "\n",
    "y = np.log(data.Salary)\n",
    "print(y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state = 127)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Estandarizamos las features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scl = scaler.fit_transform(X_train)\n",
    "\n",
    "X_test_scl = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Observación: Hasta acá repetimos exactamente los mismo pasos que hicimos en la clase de modelos de ensamble.\n",
    "\n",
    "Ahora creamos el modelos de boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "base_regressor = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "bost_linreg = AdaBoostRegressor(base_estimator = base_regressor, \n",
    "                            n_estimators = 200,\n",
    "                            learning_rate = 0.8,           \n",
    "                            loss = 'linear',\n",
    "                            random_state = 127)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostRegressor(base_estimator=LinearRegression(), learning_rate=0.8,\n",
       "                  n_estimators=200, random_state=127)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bost_linreg.fit(X_train_scl, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Evaluamos la performance en test mediante el error cuadrático medio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43887741154489757"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = bost_linreg.predict(X_test_scl)\n",
    "performance = mean_squared_error(y_test, prediction)\n",
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Gradient Boosting\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_38_Boosting/M5_CLASE_38_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introducción\n",
    "\n",
    "---\n",
    "\n",
    "<table><tr><td style=\"font-size:16px;width:55%;line-height:2;\"><ul> \n",
    "<li>\n",
    "\n",
    "Gradient boosting es un algoritmo greedy que puede sobreajustar rápidamente al dataset de entrenamiento \n",
    "</li>\n",
    "<li>    \n",
    "Puede mejorar usando regularización\n",
    "</li>\n",
    "<li>    \n",
    "Gradient boosting resuelve problemas de <b>aprendizaje supervisado</b>, que interpretan al boosting como un <b>problema de optimización, generando una función de pérdida y que buscan optimizar</b>. \n",
    "</li>\n",
    "<li>    \n",
    "Entrenamos <b>modelos débiles de forma secuencial</b> para ir minimizando la función de pérdida.\n",
    "</li>\n",
    "<li>    \n",
    "La pérdida está representada por los residuos. \n",
    "</li>\n",
    "<li>     \n",
    "A <b>diferencia de Adaboost, el resultado incorrecto no recibe un mayor peso en gradient boosting</b>. En cambio, se minimiza la función de pérdida de los modelos débiles al obtener promedios de las predicciones.\n",
    "</li> \n",
    "    \n",
    "    \n",
    "    \n",
    "</ul>    \n",
    "</td>    \n",
    "<td>    \n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_38_Boosting/M5_CLASE_38_visualizing_gradient_boosting.jpg\" align=\"center\"/>\n",
    "</td>        \n",
    "</tr>\n",
    "</table>\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "El objetivo de cualquier problema de aprendizaje supervisado es el de definir una función de pérdida y minimizarla. \n",
    "\n",
    "En el caso de regresión, por ejemplo tenemos el ECM, definido como:\n",
    "\n",
    "$$Loss = MSE = \\frac{1}{n} \\sum(y_i - \\hat{y_i})^2$$\n",
    "\n",
    "donde \n",
    "\n",
    "$y_i$: es el valor target de la instancia i-ésima\n",
    "\n",
    "$\\hat{y_i}$: es el valor predicho por el modelo para la instancia i-ésima\n",
    "\n",
    "$L(y_i, \\hat{y_i})$: es la función de pérdida\n",
    "\n",
    "\n",
    "**Queremos que nuestras predicciones minimicen nuestra función de pérdida.**\n",
    "\n",
    "Debemos optimizar las funciones de pérdida para reducir los errores relacionados con la predicción. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Usando descenso del gradiente podemos actualizar nuestras predicciones con una tasa de aprendizaje $\\alpha$ de modo tal que la suma de los residuos tienda a cero: \n",
    "\n",
    "$$\\hat{y_i} = \\hat{y_i} + \\alpha . \\frac{\\partial \\sum(y_i - \\hat{y_i})^2 }{\\partial \\hat{y_i}} $$\n",
    "\n",
    "Obteniendo\n",
    "\n",
    "$$\\hat{y_i} = \\hat{y_i} - \\alpha . 2. \\sum(y_i - \\hat{y_i}) $$\n",
    "\n",
    "Donde\n",
    "\n",
    "$\\alpha$ es la tasa de aprendizaje\n",
    "\n",
    "$\\sum(y_i - \\hat{y_i})$ es la suma de los residuos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Algoritmo gradient boosting\n",
    "\n",
    "---\n",
    "\n",
    "1. Ajustar **un modelo** utilizando a $X$ como features y a $y$ como target. Obtener las predicciones del modelo $y_{predicted}$\n",
    "\n",
    "2. Calcular los residuos: $e1 = y$ - $y_{predicted}$\n",
    "\n",
    "3. Ajustar **otro modelo** para los residuos $e1$: utilizando a $X$ como features y a $e1$ como target. Obtener las predicciones $e1_{predicted}$. \n",
    "\n",
    "4. Sumar los modelos:  $y_{predicted2} = y_{predicted} + alfa * e1_{predicted}$, donde $\\alpha$ es el learning rate.\n",
    "\n",
    "5. Ajustar **otro modelo** para los residuos $e2$: $e2 = y - y_{predicted2}$\n",
    "\n",
    "Repetir los pasos 2 a 5 hasta que el modelo comience a sobre ajustar o la suma de los residuos sea constante. \n",
    "\n",
    "El overfitting puede controlarse verificando los valores de accuracy en el conjunto de validación.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table><tr><td style=\"font-size:16px;width:45%;line-height:2;\">\n",
    "Analicémoslo paso por paso. En la primera iteración, se usa un modelo simple para ajustar a los datos. \n",
    "<br/>\n",
    "Los residuos se pueden observan en el gráfico de la derecha. La función de pérdida busca minimizar estos residuos.\n",
    "<br/>\n",
    "Luego, se agregan modelos débiles para que se concentren en las áreas donde los modelos ya existentes performan mal.\n",
    "<br/>\n",
    "Vemos como después de 3 iteraciones el ensamble de modelos débiles ya comienza a performar mejor. \n",
    "    </td><td>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_38_Boosting/M5_CLASE_38_gradient_boosting_1.png\" align=\"center\" />\n",
    "</td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table><tr><td style=\"font-size:16px;width:45%;line-height:2;\">\n",
    "Luego de 20 iteraciones el modelo ajusta muy bien a los datos y los residuos cayeron prácticamente a cero.\n",
    "</td><td>\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_38_Boosting/M5_CLASE_38_gradient_boosting_2.png\" align=\"center\" />\n",
    "</td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table><tr><td style=\"font-size:16px;width:45%;line-height:2;\">\n",
    "Luego de 50 iteraciones todos los residuos son prácticamente cero.\n",
    "</td><td>\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_38_Boosting/M5_CLASE_38_gradient_boosting_3.png\" align=\"center\" />\n",
    "</td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ejemplo\n",
    "\n",
    "---\n",
    "\n",
    "Continuemos el ejemplo anterior usando Gradient Boosting, y comparemos la performance en test con el modelo anterior.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "gb_reg = GradientBoostingRegressor(loss = 'ls',\n",
    "                                    learning_rate = 0.5,\n",
    "                                    n_estimators=200, \n",
    "                                    subsample = 1,\n",
    "                                    criterion='mse',\n",
    "                                    max_depth = 4, \n",
    "                                    random_state = 127)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(criterion='mse', learning_rate=0.5, max_depth=4,\n",
       "                          n_estimators=200, random_state=127, subsample=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_reg.fit(X_train_scl, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Evaluamos la performance en test mediante el error cuadrático medio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2624868528550717"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = gb_reg.predict(X_test_scl)\n",
    "performance = mean_squared_error(y_test, prediction)\n",
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Vemos que para este problema la perfomance que obtuvimos con gradient boosting es mejor que la obtenida con adaboost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> XGBoost\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_38_Boosting/M5_CLASE_38_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introducción\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "XGBoost son las siglas de Extreme Gradient Boosting; es una implementación específica del método Gradient Boosting que usa aproximaciones más precisas para encontrar el mejor modelo de árbol. Emplea una serie de mejoras, las mas importantes son:\n",
    "\n",
    "1. Calcula gradientes de segundo orden, es decir, segundas derivadas parciales de la función de pérdida (similar al método de Newton), que proporciona más información sobre la dirección de los gradientes y cómo llegar al mínimo de nuestra función de pérdida. Mientras que gradient boosting usa la función de pérdida de nuestro modelo base (por ejemplo, árbol de decisión) como un proxy para minimizar el error del modelo general, XGBoost usa la derivada de segundo orden como una aproximación.\n",
    "\n",
    "2. Regularización L1 y L2, que mejora la generalización del modelo.\n",
    "\n",
    "XGBoost tiene ventajas adicionales: el entrenamiento es muy rápido y se puede paralelizar / distribuir entre clústeres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "El primer release de XGBoost nace en marzo de 2014 y se empieza a emplear en competencias de Kaggle en 2015, rápidamente convirtiéndose en el algoritmo más popular entre los ganadores en problemas con datasets estructurados. \n",
    "\n",
    "Este algoritmo es una reimplementación de Gradient Boosting, con importantes mejoras que veremos en esta presentación.\n",
    "\n",
    "Fue diseñado por Tianqi Chen, de la Universidad de Washington, y rápidamente recibió numerosos colaboradores en su <a href=\"https://github.com/dmlc/xgboost\" target=\"_blank\">proyecto</a>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "¿Por qué usar XGBoost en lugar de Gradient Boosting?\n",
    "\n",
    "1- Velocidad\n",
    "\n",
    "XGBoost se impuso por lograr ajustes de modelos ampliamente más rápidos que los implementados en las librerías previamente existentes (Scikit Learn de Python, gbm de R, H20 y Spark MLLib)\n",
    "\n",
    "2- Escalabilidad\n",
    "\n",
    "La implementación de XGBoost permite entrenar usando muchísima más información ya que es muy eficiente en el uso de RAM.\n",
    "\n",
    "3- Rendimiento\n",
    "\n",
    "La capacidad de realizar una búsqueda de hiperparámetros más amplia en la misma cantidad de tiempo y de entrenar con más información, sumado a una mejora clave en la función objetivo, permite que con este algoritmo se obtengan mejores resultados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "XGBoost soporta las 3 principales implementaciones de Gradient Boosting:\n",
    "\n",
    "1- Gradient Boosting\n",
    "\n",
    "2- Stochastic Gradient Boosting, con sub-muestras en filas, columnas y columnas por split\n",
    "\n",
    "3- Gradient Boosting con regularización, con normas L1 y L2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## XGBoost - Gradient Boosting con regularización\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_38_Boosting/M5_CLASE_38_gradient_boosting_regularizacion_1.png\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_38_Boosting/M5_CLASE_38_gradient_boosting_regularizacion_2.png\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_38_Boosting/M5_CLASE_38_gradient_boosting_regularizacion_3.png\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_38_Boosting/M5_CLASE_38_gradient_boosting_regularizacion_4.png\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_38_Boosting/M5_CLASE_38_gradient_boosting_regularizacion_5.png\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_38_Boosting/M5_CLASE_38_gradient_boosting_regularizacion_6.png\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_38_Boosting/M5_CLASE_38_gradient_boosting_regularizacion_7.png\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_38_Boosting/M5_CLASE_38_gradient_boosting_regularizacion_8.png\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Algoritmos para encontrar los splits\n",
    "\n",
    "---\n",
    "\n",
    "Una de las mejoras de XGBoost es el algoritmo para encontrar los splits. XGBoost implementa tres tipos de algoritmos:\n",
    "\n",
    "* **Exact Greedy**: este algoritmo es el que empleaban previamente Scikit y GBM (R). Esencialmente es un método performante de probar todos los posibles puntos de split y quedarse con el mejor de todos. Este método es mucho más lento que los próximos dos.\n",
    "\n",
    "* **Aproximado**: este algoritmo no prueba todos los posibles puntos de split sino que sólo toma los percentiles de cada variable y los prueba. Se recalculan los bins en cada iteración.\n",
    "\n",
    "* **Histograma**: los valores de las variables continuas son almacenados en bins de un histograma, y reutilizados a lo largo de los cómputos. Luego, el split se hace entre los bins del histograma. Permite entre 2 y 256 bins.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mejoras adicionales\n",
    "\n",
    "---\n",
    "\n",
    "Además de las mejoras antes mencionadas, se incluyeron en el desarrollo de XGBoost:\n",
    "\n",
    "* **Shrinkage (eta)**: básicamente este parámetro quita peso a cada nuevo árbol que se agrega al ensamble, multiplicando su influencia por una constante. \n",
    "\n",
    "* **Sampling de columnas**: de la misma forma que Random Forest, esta librería implementa el sampling de columnas para reducir el overfitting e incrementar la variabilidad de los árboles. \n",
    "\n",
    "* **Sampling de filas (subsample)**: permite definir qué proporción de las filas se usan para entrenar cada árbol. \n",
    "\n",
    "* **Ausencia de datos**: a los valores nulos XGBoost le asignará una dirección por default en cada nodo. Es decir, los agrupará con los valores de la izquierda o de la derecha del split, según cuál sea la dirección que da mejores resultados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Otras implementaciones\n",
    "\n",
    "---\n",
    "\n",
    "En febrero de 2016 surge la versión beta de <a href=\"https://github.com/Microsoft/LightGBM\">LightGBM</a>, un proyecto desarrollado por Microsoft. \n",
    "\n",
    "Este algoritmo se caracteriza por ocupar mucha menos memoria RAM y ser, generalmente, mucho más rápido que el XGBoost original, obteniendo prácticamente los mismos resultados, si no mejores.\n",
    "\n",
    "LightGBM emplea un algoritmo para buscar los splits diferentes llamado GOSS (Gradient-based one-side sampling). \n",
    "\n",
    "Este algoritmo hace lo siguiente:\n",
    "\n",
    "* Para cada observación calcula el gradiente.\n",
    "\n",
    "* Se seleccionan las instancias con un gradiente elevado y se samplea un porcentaje de las instancias con un gradiente bajo. La idea es que las observaciones con un gradiente bajo ya pueden ser bien predichas.\n",
    "\n",
    "* Se entrena el árbol con este set de datos, y para evaluar los splits se multiplica por una constante las observaciones muestreadas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "En 2017 Yandex libera el proyecto de <a href=\"https://github.com/catboost\">CatBoost</a>. \n",
    "\n",
    "Este algoritmo también es muy rápido y presenta resultados excelentes sin necesidades de costosas búsquedas de hiperparámetros.\n",
    "\n",
    "* Permite transformar a one-hot encoding las variables que tengan menos valores distintos que una determinada cantidad, y tiene un manejo especial para las variables categóricas. En este segundo caso, el algoritmo implementa distintas métricas que se aplican en cada split para analizar la relación entre las categorías y el label. \n",
    "\n",
    "* Este algoritmo además implementa una alerta para detección temprana de overfitting que permite detener rápidamente el entrenamiento ante esta situación.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Conclusiones\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_38_Boosting/M5_CLASE_38_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Generalmente, la performance obtenida con modelos de boosting es mejor que con modelos de bagging.\n",
    "\n",
    "* El tiempo de entrenamiento de modelos de boosting es considerablemente mayor que el necesario para entrenar modelos de bagging.\n",
    "\n",
    "* Existen varias implementaciones de gradient boosting que mejoran los tiempos de entrenamiento y disminuyen el costo computacional.\n",
    "\n",
    "* Una de las mayores motivaciones al usar gradient boosting es que permite optimizar una función de costo especificada por el usuario, en lugar de una función de pérdida que generalmente ofrece menos control y no se corresponde esencialmente con aplicaciones del mundo real.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Hands-on\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_38_Boosting/M5_CLASE_38_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ejercicio\n",
    "\n",
    "---\n",
    "\n",
    "Vamos a usar el dataset Hitters para entrenar un modelo adaboost y un modelo gradient boosting para clasificar el valor de `Salary` en alto o bajo.\n",
    "\n",
    "#### AdaBoostClassifier\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
    "\n",
    "#### GradientBoostingClassifier\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n",
    "\n",
    "Vamos a evaluar la performance de los dos ensambles usando \n",
    "\n",
    "* AUC\n",
    "\n",
    "* Accuracy\n",
    "\n",
    "* Matriz de confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Puntos 1 a 4 son idénticos a los de la clase de Modelos de Ensamble:\n",
    "\n",
    "1. Leer los datos y, para simplificar, conservar sólo los registros completos y las features numéricas.\n",
    "\n",
    "2. Crear una variable categórica, a partir de `Salary`, de valores alto / bajo representados como 1 / 0, usando como umbral un valor de Salary igual a 600\n",
    "\n",
    "3. Crear los conjuntos de train y test\n",
    "\n",
    "4. Estandarizar las features\n",
    "\n",
    "Nuevo:\n",
    "\n",
    "5. Entrenar los modelos de boosting y evaluar AUC, accuracy y matriz de confusión\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solución\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import binarize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1 a 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(322, 20)\n",
      "(263, 20)\n",
      "(263, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AtBat</th>\n",
       "      <th>Hits</th>\n",
       "      <th>HmRun</th>\n",
       "      <th>Runs</th>\n",
       "      <th>RBI</th>\n",
       "      <th>Walks</th>\n",
       "      <th>Years</th>\n",
       "      <th>CAtBat</th>\n",
       "      <th>CHits</th>\n",
       "      <th>CHmRun</th>\n",
       "      <th>CRuns</th>\n",
       "      <th>CRBI</th>\n",
       "      <th>CWalks</th>\n",
       "      <th>PutOuts</th>\n",
       "      <th>Assists</th>\n",
       "      <th>Errors</th>\n",
       "      <th>Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>315</td>\n",
       "      <td>81</td>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>38</td>\n",
       "      <td>39</td>\n",
       "      <td>14</td>\n",
       "      <td>3449</td>\n",
       "      <td>835</td>\n",
       "      <td>69</td>\n",
       "      <td>321</td>\n",
       "      <td>414</td>\n",
       "      <td>375</td>\n",
       "      <td>632</td>\n",
       "      <td>43</td>\n",
       "      <td>10</td>\n",
       "      <td>475.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>479</td>\n",
       "      <td>130</td>\n",
       "      <td>18</td>\n",
       "      <td>66</td>\n",
       "      <td>72</td>\n",
       "      <td>76</td>\n",
       "      <td>3</td>\n",
       "      <td>1624</td>\n",
       "      <td>457</td>\n",
       "      <td>63</td>\n",
       "      <td>224</td>\n",
       "      <td>266</td>\n",
       "      <td>263</td>\n",
       "      <td>880</td>\n",
       "      <td>82</td>\n",
       "      <td>14</td>\n",
       "      <td>480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>496</td>\n",
       "      <td>141</td>\n",
       "      <td>20</td>\n",
       "      <td>65</td>\n",
       "      <td>78</td>\n",
       "      <td>37</td>\n",
       "      <td>11</td>\n",
       "      <td>5628</td>\n",
       "      <td>1575</td>\n",
       "      <td>225</td>\n",
       "      <td>828</td>\n",
       "      <td>838</td>\n",
       "      <td>354</td>\n",
       "      <td>200</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>321</td>\n",
       "      <td>87</td>\n",
       "      <td>10</td>\n",
       "      <td>39</td>\n",
       "      <td>42</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>396</td>\n",
       "      <td>101</td>\n",
       "      <td>12</td>\n",
       "      <td>48</td>\n",
       "      <td>46</td>\n",
       "      <td>33</td>\n",
       "      <td>805</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>91.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>594</td>\n",
       "      <td>169</td>\n",
       "      <td>4</td>\n",
       "      <td>74</td>\n",
       "      <td>51</td>\n",
       "      <td>35</td>\n",
       "      <td>11</td>\n",
       "      <td>4408</td>\n",
       "      <td>1133</td>\n",
       "      <td>19</td>\n",
       "      <td>501</td>\n",
       "      <td>336</td>\n",
       "      <td>194</td>\n",
       "      <td>282</td>\n",
       "      <td>421</td>\n",
       "      <td>25</td>\n",
       "      <td>750.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AtBat  Hits  HmRun  Runs  RBI  Walks  Years  CAtBat  CHits  CHmRun  CRuns  \\\n",
       "1    315    81      7    24   38     39     14    3449    835      69    321   \n",
       "2    479   130     18    66   72     76      3    1624    457      63    224   \n",
       "3    496   141     20    65   78     37     11    5628   1575     225    828   \n",
       "4    321    87     10    39   42     30      2     396    101      12     48   \n",
       "5    594   169      4    74   51     35     11    4408   1133      19    501   \n",
       "\n",
       "   CRBI  CWalks  PutOuts  Assists  Errors  Salary  \n",
       "1   414     375      632       43      10   475.0  \n",
       "2   266     263      880       82      14   480.0  \n",
       "3   838     354      200       11       3   500.0  \n",
       "4    46      33      805       40       4    91.5  \n",
       "5   336     194      282      421      25   750.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw = pd.read_csv(\"../Data/Hitters.csv\")\n",
    "print(data_raw.shape)\n",
    "data_complete = data_raw.dropna()\n",
    "print(data_complete.shape)\n",
    "\n",
    "data_columns = ['AtBat', 'Hits', 'HmRun', 'Runs', 'RBI', \n",
    "                'Walks', 'Years', 'CAtBat', 'CHits', 'CHmRun', \n",
    "                'CRuns', 'CRBI', 'CWalks', 'PutOuts', 'Assists', \n",
    "                'Errors', 'Salary']\n",
    "\n",
    "data = data_complete.loc[:, data_columns]\n",
    "print(data.shape)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "salary_cat = binarize(pd.DataFrame(data.Salary), threshold = 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(263, 16)\n",
      "(263, 1)\n"
     ]
    }
   ],
   "source": [
    "X = data.drop(\"Salary\", axis = 1)\n",
    "print(X.shape)\n",
    "\n",
    "y = salary_cat\n",
    "print(y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state = 127)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scl = scaler.fit_transform(X_train)\n",
    "\n",
    "X_test_scl = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "base_classifier = DecisionTreeClassifier(criterion='gini', \n",
    "                                    max_depth=3,\n",
    "                                    max_features=5, \n",
    "                                    random_state=159)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "boost_tree = AdaBoostClassifier(base_estimator = base_classifier, \n",
    "                            n_estimators = 200,\n",
    "                            learning_rate = 0.8,                                       \n",
    "                            random_state = 127)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=3,\n",
       "                                                         max_features=5,\n",
       "                                                         random_state=159),\n",
       "                   learning_rate=0.8, n_estimators=200, random_state=127)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_boost_tree = y_train.reshape(y_train.shape[0], )\n",
    "boost_tree.fit(X_train_scl, y_train_boost_tree)                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Evaluamos la performance en test mediante accuracy, auc y matriz de confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8181818181818182\n",
      "[[37  6]\n",
      " [ 6 17]]\n",
      "0.8948432760364003\n"
     ]
    }
   ],
   "source": [
    "prediction_bt = boost_tree.predict(X_test_scl)\n",
    "\n",
    "accuracy_bt = accuracy_score(y_test, prediction_bt)\n",
    "print(accuracy_bt)\n",
    "\n",
    "conf_mat_bt = confusion_matrix(y_test, prediction_bt)\n",
    "print(conf_mat_bt)\n",
    "\n",
    "prediction_bt_proba = boost_tree.predict_proba(X_test_scl)\n",
    "prediction_bt_class_1 = prediction_bt_proba[:, 1]\n",
    "auc_bt = roc_auc_score(y_test, prediction_bt_class_1)\n",
    "print(auc_bt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Gradient Boosting\n",
    "\n",
    "Ajusta árboles de regresión. Observemos que no recibe base_estimator como parámetro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "gb_classifier = GradientBoostingClassifier(loss = 'deviance',\n",
    "                                           learning_rate=0.6,\n",
    "                                           n_estimators = 200,\n",
    "                                           subsample=1,\n",
    "                                           criterion='mse',\n",
    "                                           random_state = 127)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='mse', learning_rate=0.6, n_estimators=200,\n",
       "                           random_state=127, subsample=1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_gb = y_train.reshape(y_train.shape[0], )\n",
    "gb_classifier.fit(X_train_scl, y_train_gb)                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Evaluamos la performance en test mediante accuracy, auc y matriz de confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8484848484848485\n",
      "[[39  4]\n",
      " [ 6 17]]\n",
      "0.9009100101112235\n"
     ]
    }
   ],
   "source": [
    "prediction_gb = gb_classifier.predict(X_test_scl)\n",
    "\n",
    "accuracy_gb = accuracy_score(y_test, prediction_gb)\n",
    "print(accuracy_gb)\n",
    "\n",
    "conf_mat_gb = confusion_matrix(y_test, prediction_gb)\n",
    "print(conf_mat_gb)\n",
    "\n",
    "prediction_gb_proba = gb_classifier.predict_proba(X_test_scl)\n",
    "prediction_gb_class_1 = prediction_gb_proba[:, 1]\n",
    "auc_gb = roc_auc_score(y_test, prediction_gb_class_1)\n",
    "print(auc_gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Referencias\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M5/CLASE_38_Boosting/M5_CLASE_38_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a href=\"https://www.statlearning.com/\">An Introduction to Statistical Learning. Cap. 8.3.4 Tree-Based Methods </a>\n",
    "\n",
    "<a href=\"https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205\" target=\"_blank\">Ensemble methods: bagging, boosting and stacking</a>\n",
    "\n",
    "#### Boosting\n",
    "\n",
    "<a href=\"https://www.youtube.com/watch?v=GM3CDQfQ4sw\" target=\"_blank\">Boosting</a>\n",
    "\n",
    "<a href=\"https://www.youtube.com/watch?v=LsK-xG1cLYA\" target=\"_blank\">StatQuest.AdaBoost, Clearly Explained</a>\n",
    " \n",
    "\n",
    "#### Gradient Boosting\n",
    "\n",
    "\n",
    "<a href=\"https://towardsdatascience.com/understanding-gradient-boosting-machines-9be756fe76ab\" target=\"_blank\">Understanding Gradient Boosting Machines</a>\n",
    "\n",
    "<a href=\"https://blog.mlreview.com/gradient-boosting-from-scratch-1e317ae4587d\" target=\"_blank\">Gradient Boosting from scratch</a>\n",
    "\n",
    "<a href=\"https://www.gormanalysis.com/blog/gradient-boosting-explained/\" target=\"_blank\">Gradient Boosting Explained</a>\n",
    "\n",
    "<a href=\"https://explained.ai/gradient-boosting/\" target=\"_blank\">How to explain gradient boosting</a>\n",
    "\n",
    "<a href=\"https://www.youtube.com/playlist?list=PLblh5JKOoLUJjeXUvUE0maghNuY2_5fY6\" target=\"_blank\">StatQuest.Gradient Boost</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "#### XGBoost\n",
    "\n",
    "<a href=\"https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/\" target=\"_blank\">A Gentle Introduction to XGBoost for Applied Machine Learning</a>\n",
    "\n",
    "\n",
    "<a href=\"https://medium.com/data-design/xgboost-hi-im-gamma-what-can-i-do-for-you-and-the-tuning-of-regularization-a42ea17e6ab6\" target=\"_blank\">xgboost: “Hi I’m Gamma. What can I do for you?” — and the tuning of regularization</a>\n",
    "\n",
    "\n",
    "<a href=\"https://medium.com/implodinggradients/benchmarking-lightgbm-how-fast-is-lightgbm-vs-xgboost-15d224568031\" target=\"_blank\">Benchmarking LightGBM: how fast is LightGBM vs xgboost?</a>\n",
    "\n",
    "<a href=\"https://www.youtube.com/watch?v=GrJP9FLV3FE\" target=\"_blank\">StatQuest.XGBoost in Python from Start to Finish</a>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dhdsblend2021] *",
   "language": "python",
   "name": "conda-env-dhdsblend2021-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
