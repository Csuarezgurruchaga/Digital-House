{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/csuarezgurruchaga/Desktop/Digital-House/CLASE_46/dsad_2021/common\n",
      "default checking\n",
      "Running command `conda list`... ok\n",
      "jupyterlab=2.2.6 already installed\n",
      "pandas=1.1.5 already installed\n",
      "bokeh=2.2.3 already installed\n",
      "seaborn=0.11.0 already installed\n",
      "matplotlib=3.3.2 already installed\n",
      "ipywidgets=7.5.1 already installed\n",
      "pytest=6.2.1 already installed\n",
      "chardet=4.0.0 already installed\n",
      "psutil=5.7.2 already installed\n",
      "scipy=1.5.2 already installed\n",
      "statsmodels=0.12.1 already installed\n",
      "scikit-learn=0.23.2 already installed\n",
      "xlrd=2.0.1 already installed\n",
      "nltk=3.5 already installed\n",
      "unidecode=1.1.1 already installed\n",
      "pydotplus=2.0.2 already installed\n",
      "pandas-datareader=0.9.0 already installed\n",
      "flask=1.1.2 already installed\n"
     ]
    }
   ],
   "source": [
    "# initial setup\n",
    "%run \"../../../common/0_notebooks_base_setup.py\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../../../common/logo_DH.png' align='left' width=35%/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_TOC\"></a> \n",
    "\n",
    "## Tabla de Contenidos\n",
    "\n",
    "[1- Introducción](#section_intro)\n",
    "\n",
    "$\\hspace{.5cm}$[1.1- El problema de la dimensionalidad](#section_dimension)\n",
    "\n",
    "$\\hspace{.5cm}$[1.2- Bag of words](#section_bag_of_words)\n",
    "\n",
    "[2- Preprocesamiento de texto](#section_NLTK)\n",
    "\n",
    "$\\hspace{.5cm}$[2.1 Tokenización](#section_token)\n",
    "\n",
    "$\\hspace{.5cm}$[2.2 Generación de vocabulario](#section_vocabulary)\n",
    "\n",
    "$\\hspace{1cm}$[2.2.1 Stopwords](#section_stopwords)\n",
    "\n",
    "$\\hspace{1cm}$[2.2.2 Stemming and Lemmatization](#section_stem_lemma)\n",
    "\n",
    "$\\hspace{.5cm}$[2.3 Ejemplo](#section_caso)\n",
    "\n",
    "$\\hspace{.5cm}$[2.4 Encoding: vectorización de los documentos](#section_encoding)\n",
    "\n",
    "[3- Singular value decomposition](#section_SVD)\n",
    "\n",
    "[4- La biblioteca de Babel](#section_babel)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_intro\"></a> \n",
    "\n",
    "### Introducción\n",
    "[Volver al índice](#section_TOC)\n",
    "\n",
    "En esta práctica trabajaremos con datos de tipo texto. Algo hemos hecho ya cuando preprocesamos datasets usando, por ejemplo, expresiones regulares. Lo que hacíamos en ese entonces era buscar algo que ya sabíamos que estaba allí. La tarea que nos ocupará ahora es diferente. Queremos usar los algoritmos de machine learning que estuvimos aprendiendo para hacer, por ejemplo, clasificación, pero con textos.\n",
    "\n",
    "Existen múltiples escenarios en los cuales querríamos hacer esto. Uno de los primeros problemas de aplicación de machine learning, de hecho, fue el de clasificación de correos como spam o no-spam. Otros casos son la clasificación de comentarios de usuarios sobre algún producto como positivos o negativos, identificar a qué sección de un diario pertenecen distintas notas, etc. En definiva, queremos extraer información sobre el contenido de los textos.\n",
    "\n",
    "Para poder implementar los modelos que ya conocemos, necesitamos representar los datos como una matriz de features. En el caso de modelos de aprendizaje supervisado, necesitamos además una etiqueta, como spam/ham.\n",
    "\n",
    "<b>¿Cómo convertirían un texto en una matriz de features?</b>\n",
    "\n",
    "<a id=\"section_dimension\"></a> \n",
    "### El problema de la dimensionalidad\n",
    "[Volver al índice](#section_TOC)\n",
    "\n",
    "Los textos son secuencias de palabras (y signos de puntuación!) y el sentido de los mismos está contenido precisamente en las estructuras semánticas que forman estos elementos combinados. Es decir que el sentido no está dado por la mera presencia de las palabras, sino por cómo se articulan.\n",
    "\n",
    "En 'La Biblioteca de Babel', Borges imagina una biblioteca compuesta por libros de tamaño fijo que contienen todas las combinaciones posibles de los 22 caracteres de un alfabeto, más el punto, la coma y el espacio. En el cuento, los libros son de 410 páginas, cada una con 40 renglones de 80 caracteres cada uno. \n",
    "\n",
    "El número de libros distintos que se pueden formar de esta manera es abismal (¿cómo lo calcularían?). Dado que los libros de la biblioteca contienen todas las combinaciones de caracteres posibles, algunos de estos libros son casi iguales: tal es el caso de aquellos que difieren sólo en un puñado de caracteres. Considerar a tales libros como diferentes sería un despropósito si lo que importa es el sentido semántico de los textos. Por otro lado, la mayoría de los libros en la biblioteca es, presumiblemente, una secuencia de caracteres sin sentido, no interpretables en ningún idioma. En la historia, sin embargo, los sujetos que habitan la biblioteca (los bibliotecarios), la recorren incansablemente en busca del libro que contenga su destino, o el de la humanidad, o aquél que sea el catálogo de la biblioteca. \n",
    "\n",
    "¿Cómo cambiaría la tarea de los bibliotecarios, si contaran con herramientas de machine learning para ordenar la biblioteca? \n",
    "\n",
    "Dejando de lado el hecho de que arruinarían el bello cuento de Borges, veamos cómo podrían implementar algunas herramientas de procesamiento de texto para dicha tarea.\n",
    "\n",
    "Podríamos empezar por no considerar *cualquier* combinación de caracteres sino solamente aquellas que forman palabras dentro de un vocabulario. El número de textos posibles se reduciría enormemente, pero seguiría siendo muy grande. Esto es importante porque la tarea de organizar la biblioteca implica comparar textos, ver cuáles son más parecidos entre sí que respecto a otros. Esto es, buscar una estructura en el *espacio de los textos posibles*. El problema es que de acuerdo a lo discutido más arriba, ese espacio es de una dimensionalidad enorme. Para poder computar métricas de distancia entre los textos, para poder entrenar algoritmos de machine learning que nos permitan encontrar patrones en los datos, necesitamos definir representaciones reducidas de los textos.\n",
    "\n",
    "\n",
    "<a id=\"section_bag_of_words\"></a> \n",
    "### Bag of words\n",
    "[Volver al índice](#section_TOC)\n",
    "\n",
    "Una de las maneras más simples y efectivas de representar los textos es la que se conoce como \"bag of words\" y consiste en descartar la mayor parte de la estructura de los textos como párrafos, capítulos, etc, y conservar únicamente el conjunto de palabras y el número de veces que aparecen en el texto. Es decir, olvidamos el orden en que aparecen. Matemáticamente, el número de maneras en que podemos ordenar n elementos se calcula como n! (se lee 'factorial de n' o 'n factorial') y vale:\n",
    "\n",
    "$n!=n*(n-1)*(n-2)*(n-3)*...*3*2*1$\n",
    "\n",
    "Es decir que si tuviéramos 10 palabras diferentes para construir un texto (una oración), podríamos construir $3628800$ textos distintos ya que\n",
    "\n",
    "$10!=10*9*8*7*6*5*4*3*2*1 = 3628800 $ \n",
    "\n",
    "Pero en el esquema \"bag of words\" todos ellos estarían representados de la misma manera (la misma bolsa de palabras), de manera que serían textos indistinguibles (y el bibliotecario de babel los ubicaría en la misma sección).\n",
    "\n",
    "Antes de seguir desambigüemos un poco la jerga. Llamaremos **corpus** a un conjunto de textos (por ejemplo la biblioteca de Babel) y **documento** a cada texto que compone el corpus (puede ser un libro, un twit o el comentario de un usuario) y que a la vez es nuestra unidad de dato (sería una fila en un dataframe).\n",
    "\n",
    "Computar la representación \"bag of words\" de un corpus de documentos conlleva tres pasos:\n",
    "\n",
    "1. Tokenización: convertir cada documento a una lista de palabras (y signos de puntuación) que lo componen.\n",
    "2. Construcción de un vocabulario: colectar todas las palabras que se registraron en el corpus y ordenarlas (típicamente por orden alfabético).\n",
    "3. Encoding: representar los documentos como vectores en el espacio de las palabras del vocabulario.\n",
    "\n",
    "Veremos cada uno de estos pasos en detalle y luego incorporaremos herramientas de sklearn que permiten aglutinar todo en un sólo modelo.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"section_NLTK\"></a>\n",
    "### Preprocesamiento de texto\n",
    "[Volver al índice](#section_TOC)\n",
    "\n",
    "\n",
    "https://www.nltk.org\n",
    "\n",
    "NLTK (Natural Language Toolkit) es una librería de python de código abierto para el procesamiento de lenguaje natural. Tiene un libro online gratuito para consultar:\n",
    "\n",
    "Steven Bird, Ewan Klein, and Edward Loper (2009). Natural Language Processing with Python. O’Reilly Media Inc. http://www.datascienceassn.org/sites/default/files/Natural%20Language%20Processing%20with%20Python.pdf\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! conda install --yes nltk==3.4.5\n",
    "# ! conda install --yes unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_token\"></a>\n",
    "\n",
    "### Tokenización\n",
    "[Volver al índice](#section_TOC)\n",
    "\n",
    "\n",
    "Tokenización es la transformación de un texto a unidades constitutivas llamadas tokens. Los tokens son típicamente las palabras y signos de puntuación. Podríamos intentar hacer esto mediante métodos de la clase string que ya conocemos, por ejemplo usando split(' ') para partir el string en los espacios en blanco.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pythoners are very intelligent and work very pythonly and now they are pythoning their way to success. \n",
      "\n",
      "['Pythoners', 'are', 'very', 'intelligent', 'and', 'work', 'very', 'pythonly', 'and', 'now', 'they', 'are', 'pythoning', 'their', 'way', 'to', 'success.']\n"
     ]
    }
   ],
   "source": [
    "sentence=\"Pythoners are very intelligent and work very pythonly and now they are pythoning their way to success.\"\n",
    "print(sentence,'\\n')\n",
    "print(sentence.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizers\n",
    "\n",
    "Como vemos esto tiene el problema de que no separa las palabras de los signos de puntuación, como se nota en el último token: \"success.\"\n",
    "\n",
    "La librería NLTK cuenta con herramientas un poco más sofisticadas para hacer esto, identificando los signos de puntuación y cuándo los mismos separan oraciones y cuándo cumplen otra función, como una abreviatura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pythoners', 'are', 'very', 'intelligent', 'and', 'work', 'very', 'pythonly', 'and', 'now', 'they', 'are', 'pythoning', 'their', 'way', 'to', 'success', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Por otro lado, si quisiéramos obtener una lista de oraciones usando split obtendríamos problemas similares:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oración original:\n",
      "This is the first sentence. A gallon of milk in the U.S. costs $2.99. Is this the third sentence? Yes, it is! \n",
      "\n",
      "Tokenizamos usando split:\n",
      "['This is the first sentence', ' A gallon of milk in the U', 'S', ' costs $2', '99', ' Is this the third sentence? Yes, it is!'] \n",
      "\n",
      "Tokenizamos usando sent_tokenize:\n",
      "['This is the first sentence.', 'A gallon of milk in the U.S. costs $2.99.', 'Is this the third sentence?', 'Yes, it is!']\n"
     ]
    }
   ],
   "source": [
    "text = \"This is the first sentence. A gallon of milk in the U.S. costs $2.99. Is this the third sentence? Yes, it is!\"\n",
    "print('Oración original:')\n",
    "print(text,'\\n')\n",
    "\n",
    "\n",
    "sentences=text.split('.')\n",
    "\n",
    "print('Tokenizamos usando split:')\n",
    "print(sentences,'\\n')\n",
    "\n",
    "print('Tokenizamos usando sent_tokenize:')\n",
    "print(sent_tokenize(text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es posible que en algunos casos el tokenizador falle, por ejemplo no reconociendo una abreviatura. En el siguiente ejemplo el tokenizador no reconoce 'al.' de modo que interpreta el punto como el fin de una oración.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['According to Hotho et al.',\n",
       " '(2005) we can differ three different perspectives of text mining, namely text mining as information extraction, text mining as text data mining, and text mining as KDD (Knowledge Discovery in Databases) process.',\n",
       " \"Text mining is 'the discovery by computer of new, previously unknown information, by automatically extracting information from different written resources.\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EJEMPLO EN DONDE FALLA sent_tokenizer\n",
    "text2=\"According to Hotho et al. (2005) we can differ three different perspectives of text mining, namely text mining as information extraction, text mining as text data mining, and text mining as KDD (Knowledge Discovery in Databases) process. Text mining is 'the discovery by computer of new, previously unknown information, by automatically extracting information from different written resources.\"\n",
    "tokenized_2=sent_tokenize(text2);\n",
    "tokenized_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos *tunear* el tokenizador incluyendo abreviaturas *a piacere*. Para ello, en lugar de importar la función sent_tokenize, instanciamos los objetos PunkTrainer y PunktSentenceTokenizer. La idea de la siguiente celda no es explicar en detalle estos objetos, sino mostrarles que existen y referirlos [aquí](https://nlpforhackers.io/splitting-text-into-sentences/) para más información.\n",
    "\n",
    "(Si la url anterior no está accesible pueden consultar http://www.nltk.org/api/nltk.tokenize.html?highlight=tokenizer#nltk.tokenize.punkt.PunktTrainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['According to Hotho et al. (2005) we can differ three different perspectives of text mining, namely text mining as information extraction, text mining as text data mining, and text mining as KDD (Knowledge Discovery in Databases) process.',\n",
       " \"Text mining is 'the discovery by computer of new, previously unknown information, by automatically extracting information from different written resources.\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('punkt')\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
    "trainer = PunktTrainer()\n",
    "trainer.INCLUDE_ALL_COLLOCS = True\n",
    "tokenizer = PunktSentenceTokenizer(trainer.get_params())\n",
    "tokenizer._params.abbrev_types.add('al')\n",
    "tokenized_text2=tokenizer.tokenize(text2);\n",
    "tokenized_text2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_vocabulary\"></a> \n",
    "\n",
    "### Generación del vocabulario\n",
    "[Volver al índice](#section_TOC)\n",
    "\n",
    "\n",
    "Dijimos que vamos a representar los textos como una bolsa de palabras. Es decir que podríamos tokenizar todos los documentos y definir el vocabulario como el set de palabras que aparecieron al menos una vez en todo el corpus.\n",
    "Esto tiene el problema de que el número de palabras será muy grande y muchas de ellas serán muy poco informativas sobre el contenido del texto, por ejemplo las preposiciones, pronombres, etc. A estas palabras se las llama stopwords y a menudo se las excluye del vocabulario. Además, otra técnica para reducir la dimensionalidad del problema consiste en agrupar palabras que comparten la misma raíz etimológica como \"correr\", \"corriendo\", \"corredor\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_stopwords\"></a> \n",
    "\n",
    "#### Stopwords\n",
    "[Volver al índice](#section_TOC)\n",
    "\n",
    "NLTK tiene listas de stopwords en distintos idiomas, podemos acceder a las mismas del siguiente modo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Las primeras 20 en español:\n",
      "['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un', 'para', 'con', 'no', 'una', 'su', 'al', 'lo']\n",
      "\n",
      " Las primeras 20 en ingles:\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/csuarezgurruchaga/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords \n",
    "stopwords_sp = stopwords.words('spanish');\n",
    "\n",
    "print('\\n Las primeras 20 en español:')\n",
    "print(stopwords_sp[:20])\n",
    "\n",
    "stopwords_en=stopwords.words('english');\n",
    "\n",
    "print('\\n Las primeras 20 en ingles:')\n",
    "print(stopwords_en[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_stem_lemma\"></a> \n",
    "\n",
    "#### Stemming y Lemmatization \n",
    "[Volver al índice](#section_TOC)\n",
    "\n",
    "\n",
    "Stemming y lemmatization son maneras de reducir las palabras a su raíz etimológica. En el caso de stemming, esto se hace por sustracción de sufijos y prefijos de las palabras. La raíz que queda (stem) muchas veces no es una palabra en sí misma. Por ejemplo al pasar la palabra \"corriendo\" por un stemmer obtenemos \"corr\". \n",
    "\n",
    "El lematizador, por el contrario, siempre devuelve una versión reducida de la palabra (lema), pero que es en sí misma una palabra de la misma familia.\n",
    "\n",
    "Para profundizar:\n",
    "https://www.datacamp.com/community/tutorials/stemming-lemmatization-python\n",
    "\n",
    "#### Stemming\n",
    "\n",
    "Para el idioma inglés, podemos elegir entre PorterStammer o LancasterStammer, siendo PorterStemmer el más antiguo desarrollado originalmente en 1979. LancasterStemmer se desarrolló en 1990 y utiliza un enfoque más agresivo que el algoritmo de stemming de Porter. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos los algoritmos de Stemming\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer\n",
      "cat\n",
      "troubl\n",
      "troubl\n",
      "troubl\n",
      "\n",
      "\n",
      "Lancaster Stemmer\n",
      "cat\n",
      "troubl\n",
      "troubl\n",
      "troubl\n",
      "\n",
      "\n",
      "Word                Porter Stemmer      lancaster Stemmer   \n",
      "friend              friend              friend              \n",
      "friendship          friendship          friend              \n",
      "friends             friend              friend              \n",
      "friendships         friendship          friend              \n",
      "stabil              stabil              stabl               \n",
      "destabilize         destabil            dest                \n",
      "misunderstanding    misunderstand       misunderstand       \n",
      "railroad            railroad            railroad            \n",
      "moonlight           moonlight           moonlight           \n",
      "football            footbal             footbal             \n"
     ]
    }
   ],
   "source": [
    "# Creamos objects a partir de las clases PorterStemmer y LancasterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "lancaster=LancasterStemmer()\n",
    "\n",
    "\n",
    "# Le pasamos palabras a ambos algoritmos para que hagan stemming:\n",
    "print(\"Porter Stemmer\")\n",
    "print(porter.stem(\"cats\"))\n",
    "print(porter.stem(\"trouble\"))\n",
    "print(porter.stem(\"troubling\"))\n",
    "print(porter.stem(\"troubled\"))\n",
    "print(\"\\n\")\n",
    "print(\"Lancaster Stemmer\")\n",
    "print(lancaster.stem(\"cats\"))\n",
    "print(lancaster.stem(\"trouble\"))\n",
    "print(lancaster.stem(\"troubling\"))\n",
    "print(lancaster.stem(\"troubled\"))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Creamos una lista de palabras para hacer stemming con ambos algoritmos:\n",
    "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\",\"stabil\",\"destabilize\",\"misunderstanding\",\"railroad\",\"moonlight\",\"football\"]\n",
    "print(\"{0:20}{1:20}{2:20}\".format(\"Word\",\"Porter Stemmer\",\"lancaster Stemmer\"))\n",
    "for word in word_list:\n",
    "    print(\"{0:20}{1:20}{2:20}\".format(word,porter.stem(word),lancaster.stem(word)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemmers en otros idiomas:\n",
    "\n",
    "Python **nltk** no solo proporciona dos stemmers en inglés: PorterStemmer y LancasterStemmer, sino también muchos stemmers que no están en inglés como **SnowballStemmers**.\n",
    "\n",
    "Idiomas que maneja **SnowballStemmers**:\n",
    "\n",
    " - danés\n",
    " - holandés\n",
    " - inglés\n",
    " - francés\n",
    " - alemán\n",
    " - húngaro\n",
    " - italiano\n",
    " - noruego\n",
    " - portugués\n",
    " - rumano\n",
    " - ruso\n",
    " - español\n",
    " - sueco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "having         have      \n",
      "corriendo      corr      \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "word='having';\n",
    "englishStemmer=SnowballStemmer(\"english\")\n",
    "print(\"{0:15}{1:10}\".format(word, englishStemmer.stem(word)))\n",
    "\n",
    "palabra='corriendo';\n",
    "spanishStemmer=SnowballStemmer(\"spanish\")\n",
    "print(\"{0:15}{1:10}\".format(palabra, spanishStemmer.stem(palabra)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defimimos una función que aplica stemming a una frase en castellano:\n",
    "def stemfraseesp(frase):    \n",
    "    token_words=word_tokenize(frase)\n",
    "    token_words\n",
    "    stem_sentence=[]    \n",
    "    spanishStemmer=SnowballStemmer(\"spanish\",ignore_stopwords=True)\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(spanishStemmer.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "una fras de prueb en castellan para aplic stemming \n"
     ]
    }
   ],
   "source": [
    "frase1 = \"Una frase de prueba en castellano para aplicar stemming\"\n",
    "\n",
    "x=stemfraseesp(frase1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization en NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/csuarezgurruchaga/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word                Lemma               Stem                \n",
      "\n",
      "He                  He                  he                  \n",
      "was                 wa                  was                 \n",
      "running             running             run                 \n",
      "and                 and                 and                 \n",
      "eating              eating              eat                 \n",
      "at                  at                  at                  \n",
      "same                same                same                \n",
      "time                time                time                \n",
      "He                  He                  he                  \n",
      "has                 ha                  has                 \n",
      "bad                 bad                 bad                 \n",
      "habit               habit               habit               \n",
      "of                  of                  of                  \n",
      "swimming            swimming            swim                \n",
      "after               after               after               \n",
      "playing             playing             play                \n",
      "long                long                long                \n",
      "hours               hour                hour                \n",
      "in                  in                  in                  \n",
      "the                 the                 the                 \n",
      "Sun                 Sun                 sun                 \n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re # Libreria para editar texto mediante expresiones regulares\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "sentence=re.sub(r'[.]','',sentence) ; # Removemos los puntos antes de tokenizar. Usamos una expresión regular que reemplaza los '.' por un string vacio ''\n",
    "\n",
    "sentence_words = word_tokenize(sentence);\n",
    "print(\"\\n{0:20}{1:20}{2:20}\\n\".format(\"Word\",\"Lemma\",'Stem'))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}{2:20}\".format(word,wordnet_lemmatizer.lemmatize(word),englishStemmer.stem(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_caso\"></a> \n",
    "\n",
    "#### Antes de seguir, apliquemos lo visto hasta aquí a un pequeño corpus\n",
    "\n",
    "[Volver al índice](#section_TOC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos textos \n",
    "\n",
    "t0 = \"El potro y el angel llegaron al cine por casualidad.\"\n",
    "t1 = \"El ángel, el tanque del cine nacional, un paso más cerca del oscar\"\n",
    "t2 = \"final del mes del cine nacional: 'El Potro', la única cinta 'millonaria'\"\n",
    "t3 = \"Juan Martin del potro volvió a tandil: se dio el ultimo baño de masas con los suyos.\"\n",
    "t4 = \"Juan Martin del potro fue recibido por una multitud en Tandil.\"\n",
    "t5=  \"Juan Martin del potro fue a ver 'El Potro' al cine y le encantó.\"\n",
    "textos=[t0,t1,t2,t3,t4,t5];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vamos a remover signos de puntuación, llevar todo el texto a minúsculas e ignorar las tildes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el potro y el angel llegaron al cine por casualidad\n",
      "el angel el tanque del cine nacional un paso mas cerca del oscar\n",
      "final del mes del cine nacional el potro la unica cinta millonaria\n",
      "juan martin del potro volvio a tandil se dio el ultimo bano de masas con los suyos\n",
      "juan martin del potro fue recibido por una multitud en tandil\n",
      "juan martin del potro fue a ver el potro al cine y le encanto\n"
     ]
    }
   ],
   "source": [
    "import unidecode\n",
    "import re\n",
    "\n",
    "textos_limpios=[];\n",
    "for t in textos:\n",
    "    t_lower_no_accents=unidecode.unidecode(t.lower()); # sacamos acentos y llevamos a minuscula\n",
    "    t_lower_no_accents_no_punkt=re.sub(r'([^\\s\\w]|_)+','',t_lower_no_accents); # quitamos signos de puntuacion usando una regex que reemplaza todo lo q no sean espacios o palabras por un string vacio\n",
    "    print(t_lower_no_accents_no_punkt)\n",
    "    textos_limpios.append(t_lower_no_accents_no_punkt)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizamos y removemos stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['potro', 'angel', 'llegaron', 'cine', 'casualidad']\n",
      "['angel', 'tanque', 'cine', 'nacional', 'paso', 'mas', 'cerca', 'oscar']\n",
      "['final', 'mes', 'cine', 'nacional', 'potro', 'unica', 'cinta', 'millonaria']\n",
      "['juan', 'martin', 'potro', 'volvio', 'tandil', 'dio', 'ultimo', 'bano', 'masas']\n",
      "['juan', 'martin', 'potro', 'recibido', 'multitud', 'tandil']\n",
      "['juan', 'martin', 'potro', 'ver', 'potro', 'cine', 'encanto']\n"
     ]
    }
   ],
   "source": [
    "# Tokenizamos los textos, removiendo stopwords\n",
    "tokenized_texts=[];\n",
    "for t in textos_limpios:\n",
    "    tokens=word_tokenize(t);\n",
    "    tokens_without_stopwords=[tok for tok in tokens if not (tok in stopwords_sp)]; # Creamos una lista de tokens que no pertenecen a la lista de stopwords\n",
    "    print(tokens_without_stopwords)\n",
    "    tokenized_texts.append(tokens_without_stopwords);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['angel', 'bano', 'casualidad', 'cerca', 'cine', 'cinta', 'dio', 'encanto', 'final', 'juan', 'llegaron', 'martin', 'mas', 'masas', 'mes', 'millonaria', 'multitud', 'nacional', 'oscar', 'paso', 'potro', 'recibido', 'tandil', 'tanque', 'ultimo', 'unica', 'ver', 'volvio']\n"
     ]
    }
   ],
   "source": [
    "# Construimos el vocabulario como el set de tokens conservados\n",
    "vocabulary=sorted(set([tok for tokens in tokenized_texts for tok in tokens]));\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_encoding\"></a>\n",
    "\n",
    "### Encoding: representación vectorial de los documentos\n",
    "[Volver al índice](#section_TOC)\n",
    "\n",
    "De acuerdo a lo discutido en la introducción, en el esquema \"Bag of words\" podemos representar a los documentos en función de qué palabras los componen, sin considerar las estructuras semánticas que éstas forman. De esta manera, un documento puede representarse como un vector en el espacio de palabras que conforman el vocabulario. Existen diferentes maneras de definir estos vectores. La más intuitiva es contar el número de veces que aparece cada palabra en un documento y asignarlo como la coordenada o el peso correspondiente a dicha palabra en el vector. Veamoslo en un ejemplo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>angel</th>\n",
       "      <th>bano</th>\n",
       "      <th>casualidad</th>\n",
       "      <th>cerca</th>\n",
       "      <th>cine</th>\n",
       "      <th>cinta</th>\n",
       "      <th>dio</th>\n",
       "      <th>encanto</th>\n",
       "      <th>final</th>\n",
       "      <th>juan</th>\n",
       "      <th>...</th>\n",
       "      <th>oscar</th>\n",
       "      <th>paso</th>\n",
       "      <th>potro</th>\n",
       "      <th>recibido</th>\n",
       "      <th>tandil</th>\n",
       "      <th>tanque</th>\n",
       "      <th>ultimo</th>\n",
       "      <th>unica</th>\n",
       "      <th>ver</th>\n",
       "      <th>volvio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>t0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    angel  bano  casualidad  cerca  cine  cinta  dio  encanto  final  juan  \\\n",
       "t0      1     0           1      0     1      0    0        0      0     0   \n",
       "t1      1     0           0      1     1      0    0        0      0     0   \n",
       "t2      0     0           0      0     1      1    0        0      1     0   \n",
       "t3      0     1           0      0     0      0    1        0      0     1   \n",
       "t4      0     0           0      0     0      0    0        0      0     1   \n",
       "t5      0     0           0      0     1      0    0        1      0     1   \n",
       "\n",
       "    ...  oscar  paso  potro  recibido  tandil  tanque  ultimo  unica  ver  \\\n",
       "t0  ...      0     0      1         0       0       0       0      0    0   \n",
       "t1  ...      1     1      0         0       0       1       0      0    0   \n",
       "t2  ...      0     0      1         0       0       0       0      1    0   \n",
       "t3  ...      0     0      1         0       1       0       1      0    0   \n",
       "t4  ...      0     0      1         1       1       0       0      0    0   \n",
       "t5  ...      0     0      2         0       0       0       0      0    1   \n",
       "\n",
       "    volvio  \n",
       "t0       0  \n",
       "t1       0  \n",
       "t2       0  \n",
       "t3       1  \n",
       "t4       0  \n",
       "t5       0  \n",
       "\n",
       "[6 rows x 28 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "Encoded_data=np.zeros((len(tokenized_texts),len(vocabulary)),dtype='int')\n",
    "for i,text in enumerate(tokenized_texts):\n",
    "    for word in text:\n",
    "        Encoded_data[i,vocabulary.index(word)]+=1 # contamos\n",
    "Encoded_data=pd.DataFrame(Encoded_data,columns=vocabulary,index=['t'+str(i) for i in range(len(tokenized_texts))]);\n",
    "Encoded_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer\n",
    "\n",
    "Todo lo anterior lo podemos hacer en un sólo paso con la herramienta CountVectorizer de scikit-learn. \n",
    "\n",
    "\n",
    "Notar que al momento de instanciarla, nos permite pasarle una lista de stopwords para que no las tenga en cuenta.\n",
    "Como todos los modelos de sklearn, tiene los métodos fit y transform. En este caso \"fit\" genera el vocabulario a partir de los documentos, y \"transform\" vectoriza los documentos al espacio del vocabulario.\n",
    "\n",
    "Como típicamente el vocabulario es muy grande, las matrices son muy esparsas (tienen muchos ceros) por lo que es conveniente almacenar sólamente las entradas no nulas de las mismas en un objeto de la clase \"sparse matrix\". CountVectorizer genera matrices de esta clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario:\n",
      " {'potro': 19, 'angel': 0, 'llegaron': 10, 'cine': 4, 'casualidad': 2, 'tanque': 23, 'nacional': 16, 'paso': 18, 'cerca': 3, 'oscar': 17, 'final': 8, 'mes': 13, 'unica': 25, 'cinta': 5, 'millonaria': 14, 'juan': 9, 'martin': 11, 'volvio': 27, 'tandil': 22, 'dio': 6, 'ultimo': 24, 'bano': 1, 'masas': 12, 'suyos': 21, 'recibido': 20, 'multitud': 15, 'ver': 26, 'encanto': 7}\n",
      "\n",
      " Transformamos los textos a una matriz esparsa: <class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>angel</th>\n",
       "      <th>bano</th>\n",
       "      <th>casualidad</th>\n",
       "      <th>cerca</th>\n",
       "      <th>cine</th>\n",
       "      <th>cinta</th>\n",
       "      <th>dio</th>\n",
       "      <th>encanto</th>\n",
       "      <th>final</th>\n",
       "      <th>juan</th>\n",
       "      <th>...</th>\n",
       "      <th>paso</th>\n",
       "      <th>potro</th>\n",
       "      <th>recibido</th>\n",
       "      <th>suyos</th>\n",
       "      <th>tandil</th>\n",
       "      <th>tanque</th>\n",
       "      <th>ultimo</th>\n",
       "      <th>unica</th>\n",
       "      <th>ver</th>\n",
       "      <th>volvio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   angel  bano  casualidad  cerca  cine  cinta  dio  encanto  final  juan  \\\n",
       "0      1     0           1      0     1      0    0        0      0     0   \n",
       "1      1     0           0      1     1      0    0        0      0     0   \n",
       "2      0     0           0      0     1      1    0        0      1     0   \n",
       "3      0     1           0      0     0      0    1        0      0     1   \n",
       "4      0     0           0      0     0      0    0        0      0     1   \n",
       "5      0     0           0      0     1      0    0        1      0     1   \n",
       "\n",
       "   ...  paso  potro  recibido  suyos  tandil  tanque  ultimo  unica  ver  \\\n",
       "0  ...     0      1         0      0       0       0       0      0    0   \n",
       "1  ...     1      0         0      0       0       1       0      0    0   \n",
       "2  ...     0      1         0      0       0       0       0      1    0   \n",
       "3  ...     0      1         0      1       1       0       1      0    0   \n",
       "4  ...     0      1         1      0       1       0       0      0    0   \n",
       "5  ...     0      2         0      0       0       0       0      0    1   \n",
       "\n",
       "   volvio  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       1  \n",
       "4       0  \n",
       "5       0  \n",
       "\n",
       "[6 rows x 28 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# The solution is to make sure that you preprocess your stop list to make sure that it is normalised like your tokens will be, \n",
    "# and pass the list of normalised words as stop_words to the vectoriser.\n",
    "# vectorizer=CountVectorizer(stop_words=stopwords_sp,lowercase=True,strip_accents='unicode');\n",
    "\n",
    "# si no hacemos esto y usamos directo stopwords_sp, CountVectorizer devuelve un warning\n",
    "stopwords_sp_stem = [spanishStemmer.stem(x) for x in stopwords_sp]\n",
    "\n",
    "vectorizer=CountVectorizer(stop_words=stopwords_sp_stem,lowercase=True,strip_accents='unicode');\n",
    "\n",
    "vectorizer.fit(textos);\n",
    "print('Vocabulario:\\n',vectorizer.vocabulary_) # vocabulario del corpus con la frecuencia de cada término\n",
    "\n",
    "\n",
    "CV_encoding=vectorizer.transform(textos);\n",
    "print('\\n Transformamos los textos a una matriz esparsa:',type(CV_encoding))\n",
    "\n",
    "pd.DataFrame(CV_encoding.todense(),columns=vectorizer.get_feature_names()) # Usamos el método .todense() para ver la matriz completa\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Term Frequency Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "La representación de los textos generada mediante countvectorizer tiene en cuenta cuántas veces se observó cada término del vocabulario en cada documento. El corpus con el que trabajamos hasta ahora es extremadamente chico y homogéneo (6 textos de una oración cada uno), pero podría pasar que un documento fuera mucho más largo que los otros y por lo tanto el conteo de palabras diera números mucho mayores en él que en el resto de los documentos. Para corregir esta anomalía, deberíamos normalizar (dividir) el conteo de cada palabra por el tamaño de cada documento.\n",
    "\n",
    "Pero además de hacer esto, hay palabras que aparecen en muchos documentos y resultan entonces poco informativas para distinguirlos. Una palabra que aparece muchas veces en un documento, pero pocas veces en los demás, es una palabra muy distintiva de ese documento y será importante para representarlo, mientras que palabras que aparecen pocas veces, o que aparecen en muchos documentos serán menos informativas. \n",
    "\n",
    "La transformación TF-IDF tiene en cuenta estos factores de la siguiente manera. El término 't' dentro del documento 'd' tiene un coeficiente tf-idf(t,d) que es el producto de dos factores:\n",
    "\n",
    "\\begin{equation}\n",
    " \\text{tf-idf}(t,d)=\\text{tf}(t,d) \\times idf(t)\n",
    "\\end{equation}\n",
    "\n",
    "Por un lado tf(t,d) es la frecuencia de aparición de t dentro de d (normalizada). Por otro lado, idf(t) es la inverse document frecuency del término t y se calcula como:\n",
    "\\begin{equation}\n",
    "    \\text{idf}(t)=\\log{\\frac{N}{\\text{df(t)+1}}} \n",
    "\\end{equation}\n",
    "\n",
    "en donde N es el número de documentos y df(t) es el número de documentos en los que aparece el término t. Se suele sumar 1 en el denominador para no tener problemas si existe un término en el vocabulario que no aparece en ningún documento (df=0). \n",
    "\n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "document1='el potro y el angel llegaron al cine por casualidad'\n",
    "\n",
    "document2='el angel el tanque del cine nacional un paso mas cerca del oscar'\n",
    "\n",
    "document3='final del mes del cine nacional el angel la unica cinta millonaria'\n",
    "\n",
    "document4='juan martin del potro volvio a tandil se dio el ultimo bano de masas con los suyos'\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "    &\\text{tf('nacional', document1)}=\\frac{1}{10}=0.1\\\\\n",
    "    &\\text{tf('nacional', document2)}=\\frac{1}{13}=0.077\\\\\n",
    "    &\\text{tf('nacional', document3)}=\\frac{0}{12}=0\\\\\n",
    "    &\\text{tf('nacional', document4)}=\\frac{0}{17}=0 \\\\\n",
    "    &\\text{idf('nacional')}=\\log{\\frac{4}{3}}=0.288\n",
    "\\end{align}\n",
    "\n",
    "y los coeficientes tf-idf:\n",
    "\n",
    "\\begin{align}\n",
    "    &\\text{tf-idf('nacional',doc1)}=0.1\\times 0.288=0.0288 \\\\\n",
    "    &\\text{tf-idf('nacional',doc2)}=0.077\\times 0.288=0.0222 \\\\\n",
    "    &\\text{tf-idf('nacional',doc3)}=0\\times 0.288=0 \\\\\n",
    "    &\\text{tf-idf('nacional',doc4)}=0\\times 0.288=0\n",
    "\\end{align}\n",
    "\n",
    "El coeficiente tfidf para \"potro\" en el documento 1 da:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{tf-idf('potro',doc1)}= \\frac{1}{10} \\times \\log{\\frac{4}{2}}=0.07\\\\\n",
    "\\end{equation}\n",
    "\n",
    "Podemos ver que si bien \"potro\" y \"nacional\" aparecen el mismo número de veces dentro del documento 1, \"potro\" tiene un coeficiente mayor porque no aparece en ningún otro documento. \n",
    "\n",
    "#### TfidfVectorizer, TfidfTransformer\n",
    "\n",
    "En sklearn, los objetos de la clase TfidfTransformer sirven para transformar una matriz generada por CountVectorizer(). Podemos saltearnos el uso de countvectorizer y usar directamente TfidfVectorizer.\n",
    "\n",
    "A diferencia de la estimación clásica del factor idf que vimos más arriba, TfidfVectorizer y TfidfTransformer por defecto calculan una versión suavizada del idf como\n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{idf(t)}= \\log{\\frac{N+1}{df(t)+1}}+1\n",
    "\\end{equation}\n",
    "\n",
    "y luego normaliza los documentos vectorizados por su norma L2. (ver https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>angel</th>\n",
       "      <th>bano</th>\n",
       "      <th>casualidad</th>\n",
       "      <th>cerca</th>\n",
       "      <th>cine</th>\n",
       "      <th>cinta</th>\n",
       "      <th>dio</th>\n",
       "      <th>encanto</th>\n",
       "      <th>final</th>\n",
       "      <th>juan</th>\n",
       "      <th>...</th>\n",
       "      <th>paso</th>\n",
       "      <th>potro</th>\n",
       "      <th>recibido</th>\n",
       "      <th>suyos</th>\n",
       "      <th>tandil</th>\n",
       "      <th>tanque</th>\n",
       "      <th>ultimo</th>\n",
       "      <th>unica</th>\n",
       "      <th>ver</th>\n",
       "      <th>volvio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.452305</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.551581</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.327231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.282590</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.343563</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.418971</td>\n",
       "      <td>0.248559</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.418971</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.418971</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.236607</td>\n",
       "      <td>0.398826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.398826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.204329</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.398826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.355931</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.355931</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.246415</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.182353</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.355931</td>\n",
       "      <td>0.291868</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.355931</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.355931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.350859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.259643</td>\n",
       "      <td>0.506793</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.415577</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.284104</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.478888</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.331540</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.490694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.478888</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      angel      bano  casualidad     cerca      cine     cinta       dio  \\\n",
       "0  0.452305  0.000000    0.551581  0.000000  0.327231  0.000000  0.000000   \n",
       "1  0.343563  0.000000    0.000000  0.418971  0.248559  0.000000  0.000000   \n",
       "2  0.000000  0.000000    0.000000  0.000000  0.236607  0.398826  0.000000   \n",
       "3  0.000000  0.355931    0.000000  0.000000  0.000000  0.000000  0.355931   \n",
       "4  0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "5  0.000000  0.000000    0.000000  0.000000  0.284104  0.000000  0.000000   \n",
       "\n",
       "    encanto     final      juan  ...      paso     potro  recibido     suyos  \\\n",
       "0  0.000000  0.000000  0.000000  ...  0.000000  0.282590  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  ...  0.418971  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.398826  0.000000  ...  0.000000  0.204329  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.246415  ...  0.000000  0.182353  0.000000  0.355931   \n",
       "4  0.000000  0.000000  0.350859  ...  0.000000  0.259643  0.506793  0.000000   \n",
       "5  0.478888  0.000000  0.331540  ...  0.000000  0.490694  0.000000  0.000000   \n",
       "\n",
       "     tandil    tanque    ultimo     unica       ver    volvio  \n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "1  0.000000  0.418971  0.000000  0.000000  0.000000  0.000000  \n",
       "2  0.000000  0.000000  0.000000  0.398826  0.000000  0.000000  \n",
       "3  0.291868  0.000000  0.355931  0.000000  0.000000  0.355931  \n",
       "4  0.415577  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "5  0.000000  0.000000  0.000000  0.000000  0.478888  0.000000  \n",
       "\n",
       "[6 rows x 28 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "Tfidf_encoding=TfidfTransformer().fit_transform(CV_encoding);\n",
    "pd.DataFrame(Tfidf_encoding.todense(),columns=vectorizer.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_SVD\"></a>\n",
    "\n",
    "### Singular Value Decomposition (SVD)\n",
    "\n",
    "[Volver al índice](#section_TOC)\n",
    "\n",
    "SVD es una manera de reducir la dimensionalidad del corpus de texto que, a diferencia de lo que hemos hecho hasta ahora, no consiste en remover elementos (stopwords, etc), sino en encontrar combinaciones de palabras que resulten informativas y quedarnos con \"las mejores\" de éstas. \n",
    "\n",
    "Una analogía posible es la siguiente: podemos describir un rectángulo dando dos variables (features): su base y su altura. Si quisiéramos describir el rectángulo con una sola de estas features, por ejemplo la base, estaríamos perdiendo información muy relevante ya que existen rectángulos muy diferentes con la misma base. Sin embargo, si generáramos una nueva variable \"área\" igual al producto de base por altura, y describiéramos al rectángulo usando solamente el área, estaríamos reduciendo la dimensionalidad de una manera mucho más razonable, guardando más información sobre el rectángulo original que al quedarnos solo con la base.\n",
    "\n",
    "SVD es una transformación algebraica parecida a PCA (principal component analysis) que se puede usar en el contexto de text mining para encontrar combinaciones lineales de los términos que resulten informativas, de modo que podamos describir el data set con un número de combinaciones menor al número de términos que teníamos originalmente. De hecho, estas combinaciones pueden considerarse como dimensiones con *sentido semántico latente* (latent semantic dimensions), es decir, dimensiones en las que tiene sentido proyectar el dataset precisamente por su contenido semántico.\n",
    "\n",
    "El motivo por el cual podemos reducir la dimensionalidad de los textos proyectandolos a estas *latent semantic dimensions* es que muchas veces existe redundancia en el conjunto de documentos. Es decir que con palabras más o menos distintas, muchos documentos hablan de los mismos temas. En el ejemplo de los 6 textos que venimos usando \n",
    "\n",
    "\n",
    "    t0='El potro y el angel llegaron al cine por casualidad.'\n",
    "    t1= 'El ángel, el tanque del cine nacional, un paso más cerca del oscar',\n",
    "    t2= \"final del mes del cine nacional: 'El Potro', la única cinta 'millonaria'\",\n",
    "    t3= 'Juan Martin del potro volvió a tandil: se dio el ultimo baño de masas con los suyos.',\n",
    "    t4= 'Juan Martin del potro fue recibido por una multitud en Tandil.',\n",
    "    t5= \"Juan Martin del potro fue a ver 'El Potro' al cine y le encantó.\"\n",
    "\n",
    "\n",
    "hay 45 palabras distintas. (Antes redujimos el número de términos a 28 quitando stopwords). Sin embargo los textos hablan esencialmente de tres temas: hay dos películas nuevas en el cine, Del Potro visitó Tandil, Del Potro fue al cine. \n",
    "\n",
    "Esta reducción de la dimensionalidad podría mejorar la performance de un clasificador o un modelo de clustering. Por otro lado, una reducción a dos o tres dimensiones nos puede permitir visualizar los datos. Hay que tener en cuenta, sin embargo, que en general necesitaremos más dimensiones para describir correctamente el corpus y es posible que la representación en dos dimensiones no nos revele mucho sobre la estructura del dataset.\n",
    "\n",
    "Lo anterior pretende dar una intuición sobre la descomposición SVD y su aplicación al análisis de textos. Comprender la matemática involucrada excede las ambiciones de esta presentación. Para indagar en ella pueden consultar alguna de estas referencias:\n",
    "\n",
    "*Practical Text Mining and Statistical Analysis for Non-structured Text Data Applications*\n",
    "2012, Chapter 11.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Singular_value_decomposition\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Implementación de SVD para visualización\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd=TruncatedSVD(n_components=2);\n",
    "P=svd.fit_transform(Tfidf_encoding)\n",
    "\n",
    "#grafico\n",
    "color = ['m', 'g', 'r', 'c', 'b','k']\n",
    "plt.figure()\n",
    "patches = []\n",
    "\n",
    "for i,texto in enumerate(textos):\n",
    "    plt.plot(P[i,0], P[i,1], color[i]+\"o\")\n",
    "    patches.append(mpatches.Patch(color=color[i], label='t'+str(i)))\n",
    "\n",
    "plt.legend(handles=patches)\n",
    "plt.xlabel('dimension 1')\n",
    "plt.ylabel('dimension 2')\n",
    "#plt.axis([-4, 4, -4, 4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como dijimos, las dimensiones que generamos son combinaciones lineales de los términos. Podemos ver cuánto pesa cada término en la definición de estas dimensiones. Ordenemos los términos en función de cuánto pesan en cada dimensión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension 1:\n",
      "['potro' 'juan' 'martin' 'cine' 'tandil' 'encanto' 'ver' 'recibido'\n",
      " 'multitud' 'angel' 'casualidad' 'llegaron' 'nacional' 'masas' 'dio'\n",
      " 'volvio' 'suyos' 'ultimo' 'bano' 'millonaria' 'final' 'cinta' 'unica'\n",
      " 'mes' 'cerca' 'oscar' 'paso' 'tanque']\n",
      "\n",
      "\n",
      "Dimension 2:\n",
      "['angel' 'cine' 'nacional' 'tanque' 'paso' 'oscar' 'cerca' 'llegaron'\n",
      " 'casualidad' 'millonaria' 'cinta' 'final' 'mes' 'unica' 'potro' 'ver'\n",
      " 'encanto' 'masas' 'dio' 'suyos' 'ultimo' 'volvio' 'bano' 'multitud'\n",
      " 'recibido' 'martin' 'juan' 'tandil']\n"
     ]
    }
   ],
   "source": [
    "comp1,comp2=svd.components_ # coeficientes (pesos) de los términos en cada una de las dos dimensiones\n",
    "\n",
    "indices=np.argsort(comp1); # los ordenamos de menor a mayor y nos quedamos con los índices de sus posiciones en el array\n",
    "indices=indices[::-1] # invertimos para que queden ordenados de mayor a menor\n",
    "\n",
    "print('Dimension 1:')\n",
    "print(np.array(vectorizer.get_feature_names())[indices]) # Evaluamos los términos en estas posiciones\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "indices=np.argsort(comp2);\n",
    "indices=indices[::-1]\n",
    "print('Dimension 2:')\n",
    "print(np.array(vectorizer.get_feature_names())[indices])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la representación 2D de los textos se ve que hay dos grupos separados: uno que habla escencialmente de cine y otro que habla de tenis. En un escenario de aprendizaje no supervisado, podríamos encontrar estos grupos mediante un algoritmo de clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_babel\"></a>\n",
    "\n",
    "##### Antes de terminar, la cantidad de libros diferentes en la Biblioteca de Babel es: \n",
    "\n",
    "[Volver al índice](#section_TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "babel=25**(410*40*80);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(el print puede tardar y exigir un poquito a la máquina)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(babel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dhdsblend2021] *",
   "language": "python",
   "name": "conda-env-dhdsblend2021-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
