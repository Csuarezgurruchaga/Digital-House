{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/csuarezgurruchaga/Desktop/Digital-House/CLASE_47/dsad_2021/common\n",
      "default checking\n",
      "Running command `conda list`... ok\n",
      "jupyterlab=2.2.6 already installed\n",
      "pandas=1.1.5 already installed\n",
      "bokeh=2.2.3 already installed\n",
      "seaborn=0.11.0 already installed\n",
      "matplotlib=3.3.2 already installed\n",
      "ipywidgets=7.5.1 already installed\n",
      "pytest=6.2.1 already installed\n",
      "chardet=4.0.0 already installed\n",
      "psutil=5.7.2 already installed\n",
      "scipy=1.5.2 already installed\n",
      "statsmodels=0.12.1 already installed\n",
      "scikit-learn=0.23.2 already installed\n",
      "xlrd=2.0.1 already installed\n",
      "nltk=3.5 already installed\n",
      "unidecode=1.1.1 already installed\n",
      "pydotplus=2.0.2 already installed\n",
      "pandas-datareader=0.9.0 already installed\n",
      "flask=1.1.2 already installed\n"
     ]
    }
   ],
   "source": [
    "# initial setup\n",
    "%run \"../../../common/0_notebooks_base_setup.py\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../../../common/logo_DH.png' align='left' width=35%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desafío: Clasificación de artículos de diario\n",
    "\n",
    "Trabajaremos con un dataset de noticias de los diarios Clarin y Pagina12. El objetivo de la práctica será implementar un modelo que permita predecir de qué diario proviene una noticia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "import unidecode\n",
    "from nltk.corpus import stopwords \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importamos los datos\n",
    "\n",
    "* Importen los datos con pandas y generen un dataframe agregando una columna 'clase' que indique si son noticias de Clarin o de Pagina12.\n",
    "\n",
    "Las noticias de Clarin se encuentran en '../Data/clarin.csv' y las de Pagina12 en '../Data/pagina12.csv'.\n",
    "\n",
    "* Concatenen ambos data sets en un solo dataframe.\n",
    "\n",
    "* ¿Cuántas noticias tenemos de cada diario?\n",
    "\n",
    "* ¿Qué columnas tiene el dataframe? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clarin = pd.read_csv('../Data/clarin.csv')\n",
    "df_clarin['class'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p12 = pd.read_csv('../Data/pagina12.csv')\n",
    "df_p12['class'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_clarin,df_p12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'cuerpo', 'fecha_hora', 'imagen', 'resumen', 'suplemento',\n",
       "       'titulo', 'url', 'class'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noticias de clarin: 382\n",
      "Noticias de pagina: 221\n"
     ]
    }
   ],
   "source": [
    "print('Noticias de clarin:',(df['class']==0).sum())\n",
    "print('Noticias de pagina:',(df['class']==1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Limpieza\n",
    "\n",
    "#### 2.1 Faltantes\n",
    "\n",
    "A partir del dataset observamos que los campos que probablemente contengan el vocabulario relevante son \"cuerpo\", \"título\" y \"resumen\".\n",
    "\n",
    "* Saquen del análisis los registros que no tienen cuerpo o título disponible\n",
    "\n",
    "* Completen los resúmenes faltantes con una campo en blanco\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['cuerpo','titulo'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(597, 9)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['resumen'].isnull(),['resumen']]='';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['resumen'].isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Suplementos relevantes\n",
    "\n",
    "Para mejorar la clasificación es conveniente retirar las secciones donde los dos diarios utilizan un vocabulario similar y muy específico del dominio como, por ejemplo, las relacionadas a deportes.\n",
    "\n",
    "* Miren las secciones dentro de la columna 'suplemento': Ojo que hay secciones de deportes con diferente nombre por ejemplo '/deportes/futbol/'\n",
    "\n",
    "* Remuevan las noticias de deportes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(380, 9)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deportes=df['suplemento'].apply(lambda x: 'deporte' in str(x).lower());\n",
    "df.drop(index=df[deportes].index,inplace=True)\n",
    "\n",
    "df.drop(index=df[df['suplemento']=='/br/'].index,inplace=True) # Tiro articulos en portugues\n",
    "\n",
    "df.shape\n",
    "#espectaculos=df['suplemento'].apply(lambda x: 'espectaculos' in str(x).lower());\n",
    "#df.drop(index=df[espectaculos].index,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Corpus\n",
    "\n",
    "El data set tiene informacion relevante en las columnas 'título', 'resumen' y 'cuerpo', de modo que podemos generar una nueva columna que sea la concatenación de estas tres. \n",
    "\n",
    "* Generen dicha columna, que será nuestro corpus de documentos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['noticia']=df['titulo']+' '+df['resumen']+' '+df['cuerpo'];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Soja: aumentó la superficie sembrada con semil...\n",
       "1      Políticos, empresarios y periodistas en la cel...\n",
       "2      Mercado de Liniers: entrada pobre y recuperaci...\n",
       "3      Kate del Castillo cuenta su versión del encuen...\n",
       "4      La billetera móvil del Banco Nación llega a lo...\n",
       "                             ...                        \n",
       "135    Dólar a 17,78 pesos  El dólar cerró ayer a 17,...\n",
       "136    Una pesadilla de tres décadas Cavallo particip...\n",
       "137    Ministros en caravana hacia el FMI  El ministr...\n",
       "138    “Los que están hoy trabajaron conmigo” A Mauri...\n",
       "139    Centro de arte  Con diferentes propuestas artí...\n",
       "Name: noticia, Length: 380, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['noticia']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modelo\n",
    "\n",
    "### 3.1 \n",
    "\n",
    "* Vectoricen el corpus de textos resultante con CountVectorizer, removiendo stopwords. Usen el argumento strip_accents='unicode' para remover tildes del texto.\n",
    "\n",
    "Atención: las stopwords importadas de nltk contienen tildes. Elimínenlas antes de vectorizar el corpus.\n",
    "\n",
    "* ¿Cuál es la dimensión de la matriz de features?\n",
    "\n",
    "* Apliquen un modelo Naive Bayes con un split simple entre train y test. \n",
    "\n",
    "* ¿Cuál es el accuracy obtenido?  \n",
    "\n",
    "* Dibujen la matriz de confusión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (285, 22256)\n",
      "\n",
      "Test Accuracy: 0.8105263157894737\n",
      "\n",
      "Confusion Matrix:\n",
      " [[51 10]\n",
      " [ 8 26]]\n"
     ]
    }
   ],
   "source": [
    "# Excluimos stopwords\n",
    "stop_words = stopwords.words('spanish');\n",
    "stop_words=[unidecode.unidecode(word.lower()) for word in stop_words ]; # quitamos acentos\n",
    "\n",
    "Train,Test=train_test_split(df[['noticia','class']],stratify=df['class'],random_state=3);\n",
    "\n",
    "Train.reset_index(drop=True,inplace=True);\n",
    "Test.reset_index(drop=True,inplace=True);\n",
    "\n",
    "vectorizer=CountVectorizer(strip_accents='unicode',stop_words=stop_words);\n",
    "vectorizer.fit(Train['noticia']);\n",
    "\n",
    "X_train=vectorizer.transform(Train['noticia']);\n",
    "X_test=vectorizer.transform(Test['noticia']);\n",
    "\n",
    "y_train=Train['class'];\n",
    "y_test=Test['class'];\n",
    "\n",
    "NBC=MultinomialNB();\n",
    "\n",
    "NBC.fit(X_train.todense(),Train['class']);\n",
    "\n",
    "test_pred=NBC.predict(X_test.todense());\n",
    "\n",
    "print('Training set shape:',X_train.shape)\n",
    "\n",
    "print('\\nTest Accuracy:',accuracy_score(Test['class'],test_pred))\n",
    "\n",
    "print('\\nConfusion Matrix:\\n',confusion_matrix(Test['class'],test_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Optimización del modelo\n",
    "\n",
    "* Hagan una gridsearch cross validation variando el hiperparámetro alpha en el rango (0;0.1)\n",
    "\n",
    "* Vean la accuracy y la matriz de confusión obtenida con el mejor modelo, en el test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.arange(0.1,2,0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 19 candidates, totalling 57 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    3.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score: 0.8000000000000002\n",
      "best params: {'alpha': 0.6}\n",
      "\n",
      "Test set:\n",
      "\n",
      "accuracy: 0.8210526315789474\n",
      "\n",
      "confusion:\n",
      " [[51 10]\n",
      " [ 7 27]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  57 out of  57 | elapsed:    4.1s finished\n"
     ]
    }
   ],
   "source": [
    "skf=StratifiedKFold(n_splits=3,random_state=0,shuffle=True);\n",
    "\n",
    "params={'alpha':np.arange(0.1,2,0.1)};\n",
    "GS_CV=GridSearchCV(MultinomialNB(),params,cv=skf,verbose=1,n_jobs=-1);\n",
    "GS_CV.fit(X_train,y_train);\n",
    "print('best score:',GS_CV.best_score_)\n",
    "print('best params:',GS_CV.best_params_)\n",
    "\n",
    "best_model=GS_CV.best_estimator_;\n",
    "best_model.fit(X_train,y_train); # entrenamos en todo el training set\n",
    "\n",
    "print('\\nTest set:\\n')\n",
    "\n",
    "test_pred=best_model.predict(X_test);\n",
    "\n",
    "print('accuracy:',accuracy_score(Test['class'],test_pred))\n",
    "\n",
    "print('\\nconfusion:\\n',confusion_matrix(Test['class'],test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Análisis de los resultados \n",
    "\n",
    "El modelo entrenado tiene el atributo \"feature_log_prob\" que contiene el logaritmo de los coeficientes $\\theta_{yi}$, que representan la probabilidad de que el término i-ésimo pertenezca a la clase $y$.\n",
    "\n",
    "¿Cuáles son las features (palabras) que mejor separan a las dos clases?\n",
    "\n",
    "* Calculen el cociente entre los logaritmos de los coeficientes estimados para la clase \"clarin\" y para \"pagina12\". ¿Cuáles términos mustran mayor diferencia entre ambos valores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/csuarezgurruchaga/opt/anaconda3/envs/dhdsblend2021/lib/python3.8/site-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Términos representativos de Clarín:\n",
      "\n",
      "['mira' 'cafe' 'km' 'moto' 'marta' 'weinstein' 'clarin' 'capa' 'norte'\n",
      " 'entradas' 'metros' 'estrellas' 'hambre' 'cuesta' 'ciccone' 'carne' 'bar'\n",
      " 'belsunce' 'wexler' 'harvey' 'huracan' 'productor' 'canciones'\n",
      " 'enfermedad' 'filme' 'estudiantes' 'arquitectura' 'ringo' 'kim' 'video'\n",
      " 'gmail' 'fuente' 'ushuaia' 'guzman' 'usuarios' 'boudou' 'medica'\n",
      " 'hotmail' 'pachelo' 'com' 'cigarrillos' 'sirve' 'talampaya' 'exito'\n",
      " 'brava' 'marido' 'placer' 'tortoni' 'festival' 'latinoamerica' 'cine'\n",
      " 'yanina' 'cerveza' 'segui' 'marcas' 'cartel' 'cartas' 'bueno' 'choque'\n",
      " 'robert' 'super' 'rioja' 'gi' 'excursion' 'nacio' 'wars' 'siciliani'\n",
      " 'bailando' 'star' 'codigo' 'cargando' 'autos' 'electronico' 'york'\n",
      " 'jurado' 'bienal' 'estrella' 'concurso' 'canon' 'luengo' 'bradley'\n",
      " 'disfrutar' 'pulmonar' 'maxima' 'laguna' 'gallo' 'peliculas' 'pedro'\n",
      " 'larry' 'inteligencia' '250' 'trabajando' 'deck' 'paciente' 'perros'\n",
      " 'polo' 'dosis' 'famoso' 'vender' 'triasico']\n",
      "\n",
      "Términos representativos de Página12:\n",
      "\n",
      "['trabajador' 'perotti' 'entrego' 'oct' 'monzo' 'provisorio' 'itu'\n",
      " 'loading' 'retroceso' 'lifschitz' 'trimestre' 'laus' 'intentos' 'negri'\n",
      " 'ardusso' 'rafaela' 'hoyts' 'clara' 'gendarmeria' 'maldonado'\n",
      " 'instalacion' 'tealdi' 'lujan' 'navaja' 'pj' 'newell' 'dal' 'pumas' 'ipc'\n",
      " 'spataro' 'protected' 'avila' 'fidel' 'incrementos' 'email'\n",
      " 'guerrilleros' 'masetto' 'pymes' 'macrista' 'llosa' 'perdieron'\n",
      " 'rosarinos' 'subas' 'ibarra' 'igualdad' 'macri' 'victoria' 'camau'\n",
      " 'village' 'gendarmes' 'presidencial' 'villeneuve' 'eguiguren' 'sukerman'\n",
      " 'capitalismo' 'rugby' 'laborales' 'endeudamiento' 'asalariados' 'higuera'\n",
      " 'cuarenta' 'aumentos' 'desigualdad' 'colombi' 'hospitales' 'garrahan'\n",
      " 'bloque' 'justicialista' 'saccomanno' 'vie' 'morales' 'showcase' 'arsat'\n",
      " 'tel' 'umet' 'paginai12' 'iet' 'torneo' 'acumulada' 'res' 'condori' 'sam'\n",
      " 'sab' 'cfk' 'cidh' 'canasta' 'resistencia' 'ciento' 'macrismo' 'cia'\n",
      " 'temer' 'sub' 'maxi' 'tano' 'che' '2d' 'dir' 'cast' 'hs' 'guevara']\n"
     ]
    }
   ],
   "source": [
    "relative_importance=best_model.feature_log_prob_[0]/best_model.feature_log_prob_[1];\n",
    "# Los valores son log-prob (negativos) de modo que relative_importance < 1 implica que\n",
    "# el coeficiente asignado en la clase 0 (clarin) es mayor que en la clase 1 (pagina) y viceveras\n",
    "\n",
    "sns.distplot(relative_importance)\n",
    "plt.xlabel('Importancia Relativa')\n",
    "\n",
    "features=np.array(vectorizer.get_feature_names());\n",
    "\n",
    "indices=np.argsort(relative_importance);\n",
    "\n",
    "print('Términos representativos de Clarín:\\n')\n",
    "print(features[indices[:100]])\n",
    "\n",
    "print('\\nTérminos representativos de Página12:\\n')\n",
    "print(features[indices[-100:]])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dhdsblend2021] *",
   "language": "python",
   "name": "conda-env-dhdsblend2021-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
