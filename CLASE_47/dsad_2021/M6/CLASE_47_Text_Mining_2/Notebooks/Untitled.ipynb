{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94935f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b631b3c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bromwell High is a cartoon comedy. It ran at t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Homelessness (or Houselessness as George Carli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brilliant over-acting by Lesley Ann Warren. Be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is easily the most underrated film inn th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is not the typical Mel Brooks film. It wa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>Towards the end of the movie, I felt it was to...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>This is the kind of movie that my enemies cont...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>I saw 'Descent' last night at the Stockholm Fi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>Some films that you pick up for a pound turn o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>This is one of the dumbest films, I've ever se...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0      Bromwell High is a cartoon comedy. It ran at t...      1\n",
       "1      Homelessness (or Houselessness as George Carli...      1\n",
       "2      Brilliant over-acting by Lesley Ann Warren. Be...      1\n",
       "3      This is easily the most underrated film inn th...      1\n",
       "4      This is not the typical Mel Brooks film. It wa...      1\n",
       "...                                                  ...    ...\n",
       "24995  Towards the end of the movie, I felt it was to...      0\n",
       "24996  This is the kind of movie that my enemies cont...      0\n",
       "24997  I saw 'Descent' last night at the Stockholm Fi...      0\n",
       "24998  Some films that you pick up for a pound turn o...      0\n",
       "24999  This is one of the dumbest films, I've ever se...      0\n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file_path = '../Data/imdb_train.zip'\n",
    "data_train = pd.read_csv(train_file_path, sep=\"\\t\")\n",
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f3a5477",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_path = '../Data/imdb_test.zip'\n",
    "data_test = pd.read_csv(test_file_path, sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c1ea149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_review(review_text, tokenizer, stemmer, stopwords):    \n",
    "    \n",
    "    #tokens (eliminamos todos los signos de puntuación)\n",
    "    words = tokenizer.tokenize(review_text)\n",
    "    #print(words)\n",
    "    \n",
    "    # stemming: raiz y minúsculas:\n",
    "    stem_words = [stemmer.stem(x) for x in words]\n",
    "    #print(stem_words)\n",
    "    \n",
    "    # eliminamos stopwords (ya pasaron por stem)\n",
    "    clean_words = [x for x in stem_words if x not in stopwords]\n",
    "    #print(clean_words)\n",
    "    \n",
    "    result = \" \".join(clean_words)\n",
    "    \n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2e9e434",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "antes:  Homelessness (or Houselessness as George Carlin stated) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. Most people think of the homeless as just a lost cause while worrying about things such as racism, the war on Iraq, pressuring kids to succeed, technology, the elections, inflation, or worrying if they'll be next to end up on the streets.<br /><br />But what if you were given a bet to live on the streets for a month without the luxuries you once had from a home, the entertainment sets, a bathroom, pictures on the wall, a computer, and everything you once treasure to see what it's like to be homeless? That is Goddard Bolt's lesson.<br /><br />Mel Brooks (who directs) who stars as Bolt plays a rich man who has everything in the world until deciding to make a bet with a sissy rival (Jeffery Tambor) to see if he can live in the streets for thirty days without the luxuries; if Bolt succeeds, he can do what he wants with a future project of making more buildings. The bet's on where Bolt is thrown on the street with a bracelet on his leg to monitor his every move where he can't step off the sidewalk. He's given the nickname Pepto by a vagrant after it's written on his forehead where Bolt meets other characters including a woman by the name of Molly (Lesley Ann Warren) an ex-dancer who got divorce before losing her home, and her pals Sailor (Howard Morris) and Fumes (Teddy Wilson) who are already used to the streets. They're survivors. Bolt isn't. He's not used to reaching mutual agreements like he once did when being rich where it's fight or flight, kill or be killed.<br /><br />While the love connection between Molly and Bolt wasn't necessary to plot, I found \"Life Stinks\" to be one of Mel Brooks' observant films where prior to being a comedy, it shows a tender side compared to his slapstick work such as Blazing Saddles, Young Frankenstein, or Spaceballs for the matter, to show what it's like having something valuable before losing it the next day or on the other hand making a stupid bet like all rich people do when they don't know what to do with their money. Maybe they should give it to the homeless instead of using it like Monopoly money.<br /><br />Or maybe this film will inspire you to help others.\n"
     ]
    }
   ],
   "source": [
    "review_text = data_train.text[1]\n",
    "\n",
    "print(\"antes: \", review_text)\n",
    "\n",
    "#eliminamos todos los signos de puntuación\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "englishStemmer = SnowballStemmer(\"english\")\n",
    "stopwords_en = stopwords.words('english');\n",
    "stopwords_en_stem = [englishStemmer.stem(x) for x in stopwords_en]\n",
    "\n",
    "review_text_clean = clean_review(review_text, tokenizer, englishStemmer, stopwords_en_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d1d5139",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train = [clean_review(x, tokenizer, englishStemmer, stopwords_en_stem) for x in data_train.text]\n",
    "#clean_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68dd28e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_test = [clean_review(x, tokenizer, englishStemmer, stopwords_en_stem) for x in data_test.text]\n",
    "#clean_test[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e5ec867",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer.fit(clean_train)\n",
    "X_train_sparse = count_vectorizer.transform(clean_train)\n",
    "X_test_sparse = count_vectorizer.transform(clean_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca2b5975",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train_sparse.todense(), \n",
    "             columns = count_vectorizer.get_feature_names()) \n",
    "y_train = data_train.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dbf76142",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.DataFrame(X_test_sparse.todense(), \n",
    "             columns = count_vectorizer.get_feature_names()) \n",
    "y_test = data_test.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a67d035",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_train, X_train_val, y_train_train, y_train_val = \\\n",
    "    train_test_split(X_train, y_train, train_size = 0.75, shuffle = True, random_state = 147)\n",
    "\n",
    "for c in [0.005, 0.008, 0.01, 0.05, 0.25, 0.5, 1]:    \n",
    "    lr = LogisticRegression(C=c, solver=\"newton-cg\", penalty=\"l2\")    \n",
    "    lr.fit(X_train_train, y_train_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_train_val, lr.predict(X_train_val))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426a28c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = LogisticRegression(C = 0.05, solver=\"newton-cg\", penalty=\"l2\")\n",
    "final_model.fit(X_train, y_train)\n",
    "print (\"Final Accuracy: %s\" \n",
    "        % accuracy_score(y_test, final_model.predict(X_test)))\n",
    "print (\"Final Confusion Matrix: \\n %s\" \n",
    "        % confusion_matrix(y_test, final_model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62bb2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_to_coef = pd.DataFrame(columns = ['word', 'coef'])\n",
    "feature_to_coef.word = count_vectorizer.get_feature_names()\n",
    "feature_to_coef.coef = final_model.coef_[0]\n",
    "feature_to_coef_sort_desc = feature_to_coef.sort_values(by = 'coef', ascending = False)\n",
    "positive_words = feature_to_coef_sort_desc.word[0:3]\n",
    "positive_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697e075b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_to_coef_sort_asc = feature_to_coef.sort_values(by = 'coef', ascending = True)\n",
    "negative_words = feature_to_coef_sort_asc.word[0:3]\n",
    "negative_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d280f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns =  np.concatenate([positive_words.values, negative_words.values])\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a3a707",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36948b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer_bigram = CountVectorizer(ngram_range = (1, 2))\n",
    "count_vectorizer_bigram.fit(clean_train)\n",
    "X_train_bigram_sparse = count_vectorizer_bigram.transform(clean_train)\n",
    "X_test_bigram_sparse = count_vectorizer_bigram.transform(clean_test)\n",
    "\n",
    "#X_train_bigram = pd.DataFrame(X_train_bigram_sparse.todense(), \n",
    "#             columns = count_vectorizer_bigram.get_feature_names()) \n",
    "             \n",
    "#X_test_bigram = pd.DataFrame(X_test_bigram_sparse.todense(), \n",
    "#             columns = count_vectorizer_bigram.get_feature_names()) \n",
    "\n",
    "#usamos las matrices esparsas porque rompe si trato de convertrlas en densas para esta cantidad de features\n",
    "X_train_bigram_train, X_train_bigram_val, y_train_train, y_train_val = \\\n",
    "    train_test_split(X_train_bigram_sparse, y_train, train_size = 0.75, shuffle = True, random_state = 147)\n",
    "\n",
    "for c in [0.01, 0.05, 0.1, 0.25, 0.5, 0.6, 0.7, 0.8, 0.9, 1]:    \n",
    "    lr = LogisticRegression(C=c, solver=\"newton-cg\", penalty=\"l2\")    \n",
    "    lr.fit(X_train_bigram_train, y_train_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_train_val, lr.predict(X_train_bigram_val))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395fe55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_bigram = LogisticRegression(C = 0.25, solver=\"newton-cg\", penalty=\"l2\")\n",
    "final_model_bigram.fit(X_train_bigram_sparse, y_train)\n",
    "print (\"Final Accuracy: %s\" \n",
    "       % accuracy_score(y_test, final_model_bigram.predict(X_test_bigram_sparse)))    \n",
    "print (\"Final Confusion Matrix: \\n %s\" \n",
    "        % confusion_matrix(y_test, final_model_bigram.predict(X_test_bigram_sparse)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25340063",
   "metadata": {},
   "source": [
    "TF-IDF es mejor que CountVectorizer porque no sólo se centra en la frecuencia de las palabras presentes en el corpus, sino que también tiene en cuenta su importancia.\n",
    "\n",
    "Volvamos a entrenar una regresión logísticas usando como features la representación tf-idf de unigramas, bigramas y trigramas.\n",
    "\n",
    "Veamos cuáles son los n-gramas más discriminantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de63045",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5aaad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer_bigram = CountVectorizer(ngram_range = (1, 2))\n",
    "count_vectorizer_bigram.fit(clean_train)\n",
    "X_train_bigram_sparse = count_vectorizer_bigram.transform(clean_train)\n",
    "X_test_bigram_sparse = count_vectorizer_bigram.transform(clean_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28795451",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2163e478",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dhdsblend2021] *",
   "language": "python",
   "name": "conda-env-dhdsblend2021-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
