{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/csuarezgurruchaga/Desktop/Digital-House/CLASE_47/dsad_2021/common\n",
      "default checking\n",
      "Running command `conda list`... ok\n",
      "jupyterlab=2.2.6 already installed\n",
      "pandas=1.1.5 already installed\n",
      "bokeh=2.2.3 already installed\n",
      "seaborn=0.11.0 already installed\n",
      "matplotlib=3.3.2 already installed\n",
      "ipywidgets=7.5.1 already installed\n",
      "pytest=6.2.1 already installed\n",
      "chardet=4.0.0 already installed\n",
      "psutil=5.7.2 already installed\n",
      "scipy=1.5.2 already installed\n",
      "statsmodels=0.12.1 already installed\n",
      "scikit-learn=0.23.2 already installed\n",
      "xlrd=2.0.1 already installed\n",
      "nltk=3.5 already installed\n",
      "unidecode=1.1.1 already installed\n",
      "pydotplus=2.0.2 already installed\n",
      "pandas-datareader=0.9.0 already installed\n",
      "flask=1.1.2 already installed\n"
     ]
    }
   ],
   "source": [
    "# initial setup\n",
    "%run \"../../../common/0_notebooks_base_setup.py\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src='../../../common/logo_DH.png' align='left' width=35%/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "<a id=\"section_clasification\"></a>\n",
    "\n",
    "# Clasificación de textos\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_toc\"></a> \n",
    "\n",
    "## Tabla de Contenidos\n",
    "\n",
    "\n",
    "[1- Dataset](#dataset)\n",
    "\n",
    "[2- Limpieza](#limpieza)\n",
    "\n",
    "[3- Vectorización y modelado](#vectorizacion)\n",
    "\n",
    "[4- Repaso del clasificador Naive Bayes](#NBC)\n",
    "\n",
    "[5- Implementaciones del modelo](#implementacion)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer,TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV,StratifiedKFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dataset\"></a>\n",
    "\n",
    "### El dataset\n",
    "\n",
    "[Volver al índice](#section_toc)\n",
    "\n",
    "\n",
    "\n",
    "Trabajaremos ahora con un dataset que contiene comentarios sobre películas hechos por usuarios del sitio IMDB.\n",
    "Además de los comentarios, los usuarios dejan un puntaje de las películas, de modo que podemos saber si los comentarios son positivos o negativos de acuerdo a dicho puntaje. A los problemas de clasificación que involucran detectar una valoración positiva o negativa en un texto se los conoce como problemas de \"sentiment analysis\". Aquí lo abordaremos como un problema de aprendizaje supervisado, dado que tenemos las etiquetas de la valoración de cada comentario.\n",
    "\n",
    "El dataset completo se encuentra aquí: http://ai.stanford.edu/~amaas/data/sentiment/.\n",
    "\n",
    "Utilizaremos una versión subsampleada del mismo como ejemplo para usar menos memoria y correr más rápido los modelos.\n",
    "\n",
    "La versión acotada se encuentra en el archivo movie_reviews.obj que podemos leer usando la librería pickle.\n",
    "El data set está guardado como un objeto de la clase Bunch de sklearn, que, como veremos, se manipula igual que un diccionario de python. Dentro de este objeto veremos que tenemos los textos almacenados en el atributo \"data\" y las etiquetas en el atributo \"target\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviews type: <class 'sklearn.utils.Bunch'>\n",
      "targets type: <class 'numpy.ndarray'>\n",
      "texts type: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "filehandler = open('../Data/movie_reviews.obj', 'rb') \n",
    "reviews = pickle.load(filehandler)\n",
    "filehandler.close()\n",
    "\n",
    "texts=reviews.data;\n",
    "targets=reviews.target;\n",
    "\n",
    "print('reviews type:',type(reviews))\n",
    "print('targets type:',type(targets))\n",
    "print('texts type:',type(texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b\"Zero Day leads you to think, even re-think why two boys/young men would do what they did - commit mutual suicide via slaughtering their classmates. It captures what must be beyond a bizarre mode of being for two humans who have decided to withdraw from common civility in order to define their own/mutual world via coupled destruction.<br /><br />It is not a perfect movie but given what money/time the filmmaker and actors had - it is a remarkable product. In terms of explaining the motives and actions of the two young suicide/murderers it is better than 'Elephant' - in terms of being a film that gets under our 'rationalistic' skin it is a far, far better film than almost anything you are likely to see. <br /><br />Flawed but honest with a terrible honesty.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Veamos como ejemplo el primer comentario\n",
    "texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nota aparte sobre el formato:\n",
    "\n",
    "En el ejemplo de arriba la 'b' antes del texto indica que los datos están codificados en bytes. Nosotros vemos el texto como una variable de tipo string, es decir una secuencia de caracteres, pero cada caracter ha sido mapeado a un número (un byte) mediante algún enconding (utf-8 por ejemplo). Es decir que si miramos el contenido de la variable como una lista, veremos los bytes. Al usar print() python decodifica los bytes a caracteres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bytes'>\n",
      "[90, 101, 114, 111, 32, 68, 97, 121, 32, 108, 101, 97, 100, 115, 32]\n",
      "b'Zero Day leads '\n"
     ]
    }
   ],
   "source": [
    "print(type(texts[0]))\n",
    "print(list(texts[0][:15]))\n",
    "print(texts[0][:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<a id=\"limpieza\"></a>\n",
    "\n",
    "#### Limpieza\n",
    "\n",
    "[Volver al índice](#section_toc)\n",
    "\n",
    "\n",
    "Vemos en el ejemplo de arriba que los comentarios tienen comandos html como $\\text{<br />}$ (break line). Vamos a removerlos antes de hacer nuestro análisis. Podríamos también llevar todo a minúsculas y remover signos de puntuación, pero como las herramientas como CountVectorizer y TfidfVectorizer ya tienen estos pasos incorporados no es necesario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b\"Zero Day leads you to think, even re-think why two boys/young men would do what they did - commit mutual suicide via slaughtering their classmates. It captures what must be beyond a bizarre mode of being for two humans who have decided to withdraw from common civility in order to define their own/mutual world via coupled destruction.  It is not a perfect movie but given what money/time the filmmaker and actors had - it is a remarkable product. In terms of explaining the motives and actions of the two young suicide/murderers it is better than 'Elephant' - in terms of being a film that gets under our 'rationalistic' skin it is a far, far better film than almost anything you are likely to see.   Flawed but honest with a terrible honesty.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [doc.replace(b\"<br />\", b\" \") for doc in texts];\n",
    "texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<a id=\"vectorizacion\"></a>\n",
    "\n",
    "#### Vectorización y Modelado\n",
    "\n",
    "[Volver al índice](#section_toc)\n",
    "\n",
    "\n",
    "Vamos a vectorizar el corpus primero con CountVectorizer y luego con TfidfVectorizer. Luego entrenaremos un clasificador de tipo Naive Bayes y evaluaremos la performance  usando distintas estrategias para reducir la dimensionalidad del dataset: remover stopwords de una lista, filtrar palabras que aparecen en muy pocos documentos.\n",
    "\n",
    "Antes de empezar, separamos el dataset haciendo un train test split. Usaremos el training set para seleccionar modelos mediante cross validation y luego reportaremos la performance del modelo elegido en el test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 18750\n",
      "test size: 6250\n",
      "proporcion de positivos en train: 0.5045866666666666\n",
      "proporcion de positivos en test: 0.48624\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train,test,y_train,y_test=train_test_split(texts,targets); # Por defecto el split se hace de manera estratificada: preservando las proporciones de los targets en train y test\n",
    "\n",
    "print('train size:',len(train))\n",
    "print('test size:',len(test))\n",
    "print('proporcion de positivos en train:',y_train.mean())\n",
    "print('proporcion de positivos en test:',y_test.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorización\n",
    "\n",
    "Usamos CountVectorizer para lleva nuestro corpus a una matriz de documentos y términos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionalidad del dataset:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(18750, 66297)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "vectorizer=CountVectorizer(strip_accents='unicode'); # Si bien los comentarios están en inglés, hay palabras con acentos por error de tipeo o por ser palabras de otro idioma. Los removemos.\n",
    "X_train=vectorizer.fit_transform(train);\n",
    "\n",
    "print('Dimensionalidad del dataset:')\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que el corpus está compuesto por más de 60 mil términos. Veamos los primeros, los últimos y algunos sampleados equiespaciados (cada 700 palabras) dentro del vocabulario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los primeros términos:\n",
      "['00', '000', '0000000000001', '00001', '00015', '000s', '001', '006', '007', '0080', '0083', '0093638', '00am', '00pm', '00s', '01', '01pm', '02', '03', '04', '041', '05', '06', '06th', '07', '08', '089', '08th', '09', '0f', '0r', '10', '100', '1000', '1000000', '1000s', '1001', '100b', '100k', '100min', '100mph', '100s', '100th', '100x', '100yards', '101', '101st', '102', '102nd', '103', '104', '1040', '1040a', '1040s', '105', '105lbs', '106', '107', '108', '109', '10am', '10lines', '10mil', '10minutes', '10p', '10pm', '10s', '10th', '10x', '10yr', '11', '110', '1100', '11001001', '1100ad', '111', '112', '1138', '114', '1146', '115', '116', '117', '11m', '11th', '12', '120', '1200', '1201', '1202', '123', '12383499143743701', '125', '127', '128', '12a', '12hr', '12m', '12mm', '12s']\n",
      "\n",
      " Los últimos términos:\n",
      "['ziyi', 'zizek', 'zizekian', 'zizte', 'zmed', 'zmeu', 'znaimer', 'zo', 'zobie', 'zod', 'zodiac', 'zodsworth', 'zoe', 'zoey', 'zohar', 'zoheb', 'zoimbies', 'zola', 'zoloft', 'zoltan', 'zombi', 'zombie', 'zombied', 'zombiefest', 'zombiefication', 'zombiefied', 'zombielike', 'zombies', 'zombiez', 'zombification', 'zombified', 'zombs', 'zomcom', 'zomcoms', 'zomcon', 'zomedy', 'zomezing', 'zone', 'zoned', 'zones', 'zonked', 'zoo', 'zooey', 'zoolander', 'zoological', 'zoologist', 'zoology', 'zoom', 'zoomed', 'zooming', 'zooms', 'zoos', 'zoot', 'zorak', 'zoran', 'zorba', 'zorich', 'zorie', 'zorrilla', 'zorro', 'zouzou', 'zowee', 'zp', 'zsa', 'zschering', 'zsigmond', 'zu', 'zubeidaa', 'zucchini', 'zucco', 'zucker', 'zuckerman', 'zucovic', 'zues', 'zuf', 'zugsmith', 'zukhov', 'zukovic', 'zulu', 'zulus', 'zumhofe', 'zungia', 'zuniga', 'zurich', 'zvezda', 'zvonimir', 'zvyagvatsev', 'zwick', 'zx81', 'zy', 'zyada', 'zyuranger', 'zz', 'zzzz', 'zzzzzzzzzzzz', 'zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz', 'zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz', 'zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz', 'æsthetic', 'østbye']\n",
      "\n",
      " Términos equiespaciados dentro del vocabulario:\n",
      "['00', '6000', 'actin', 'akhras', 'anarchic', 'aragorns', 'atlantians', 'bailout', 'bearing', 'bhand', 'bluffs', 'braggin', 'bugle', 'campus', 'caulfield', 'chenoweth', 'claus', 'comedus', 'consacrates', 'costumes', 'cruelty', 'darts', 'delventhal', 'dial', 'disorientated', 'dorn', 'dureyea', 'elite', 'entwined', 'excellente', 'famed', 'fieriest', 'fluffy', 'frenchwoman', 'gardiner', 'giulia', 'grandes', 'gushed', 'harwood', 'hes', 'hookup', 'hz', 'incite', 'instic', 'issac', 'johny', 'keighley', 'kotex', 'laudanum', 'lifted', 'lotr', 'magnus', 'marring', 'mechanisms', 'miike', 'mola', 'muder', 'naustradamous', 'nominations', 'offcourse', 'outfit', 'papapetrou', 'penalties', 'pics', 'polemics', 'premieres', 'protray', 'quinn', 'readiness', 'reinstated', 'restructuring', 'robin', 'ryo', 'scalia', 'secretly', 'shanties', 'sider', 'slipped', 'sondheim', 'spotting', 'stiflers', 'subpar', 'suxor', 'tards', 'thehollywoodnews', 'tock', 'travails', 'twinkie', 'unexpecting', 'unwritten', 'verites', 'wadd', 'wet', 'wof', 'yields']\n"
     ]
    }
   ],
   "source": [
    "print('Los primeros términos:')\n",
    "print(vectorizer.get_feature_names()[:100])\n",
    "\n",
    "print('\\n Los últimos términos:')\n",
    "print(vectorizer.get_feature_names()[-100:])\n",
    "\n",
    "print('\\n Términos equiespaciados dentro del vocabulario:')\n",
    "print(vectorizer.get_feature_names()[::700])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<a id=\"NBC\"></a>\n",
    "\n",
    "#### Breve repaso de Multinomial Naive Bayes \n",
    "\n",
    "[Volver al índice](#section_toc)\n",
    "\n",
    "Implementaremos un clasificador multinomial naive bayes.\n",
    "\n",
    "El problema de clasificación consiste en estimar una probabilidad de que la clase (Y) tome un cierto valor (y) \n",
    "dado que las features (X) tomaron los valores observados ($x_{obs}$): $P(Y=y|X=x_{obs})$.\n",
    "\n",
    "El teorema de Bayes permite descomponer esta probabilidad condicional del siguiente modo:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "P(Y=y|X=x_{obs})=\\frac{P(X=x_{obs}|Y=y)P(Y=y)}{P(X=x_{obs})}\n",
    "\\end{equation}\n",
    "\n",
    "La variable de clase Y puede tomar uno entre varios valores y a la hora de hacer una predicción elegimos el más probable:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y}=argmax_y  P(Y=y|X=x_{obs})\n",
    "\\end{equation}\n",
    "\n",
    "Esta elección es independiente del valor de $P(X=x_{obs})$ que no depende de y, de modo que sólo es necesario calcular\n",
    "\n",
    "\\begin{equation}\n",
    "P(Y=y|X=x_{obs})\\sim P(X=x_{obs}|Y=y)P(Y=y)\n",
    "\\end{equation}\n",
    "\n",
    "P(Y=y) se conoce como prior y es nuestra estimación de cuán probable es que una observación pertenezca a una clase independiendemente del valor de las features. Puede asumirse un prior uniforme (todas las clases son equiprobables) o podemos estimarlo de los datos: cuán representada está cada clase.\n",
    "\n",
    "El otro factor es la verosimilitud o likelihood $P(X=x_{obs}|Y=y)$ y depende del modelo que vayamos a usar. La palabra Naive dentro del modelo implica que podemos asumir independencia entre las features de modo que podemos factorizar esta probabilidad como\n",
    "\n",
    "\\begin{equation}\n",
    "P(Y=y|X=x_{obs})=\\prod_i P(X_i=x_i|Y=y)\n",
    "\\end{equation}\n",
    "\n",
    "en donde $X_i$ es la feature i-ésima.\n",
    "\n",
    "En el caso de clasificación de texto, los valores de las features ($x_i$) son frecuencias de aparición de palabras en los documentos, por lo que es razonable usar un modelo multinomial (Multinomial Naive Bayes) en donde cada uno de esos factores lo podemos calcular como\n",
    "\n",
    "\\begin{equation}\n",
    "P(X_i=x_i|Y=y)={\\theta_{yi}}^{x_i}\n",
    "\\end{equation}\n",
    "\n",
    "en donde $\\theta_{yi}$ es la probabilidad de que aparezca el término i-ésimo en un documento de clase $y$ y se puede estimar como\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\theta}_{yi}=\\frac{N_{yi}+\\alpha}{N_y + \\alpha n}\n",
    "\\end{equation}\n",
    "\n",
    "en donde $N_{yi}$ es el número total de veces que se observó el término i-ésimo en documentos de clase $y$, n es el número de términos, $N_y$ es el número total de ocurrencias de todos los términos dentro de la clase $y$.\n",
    "\n",
    "$\\alpha$ es un hiperparámetro del modelo que implica un suavizado de la distribución de probabilidades. Cuando $\\alpha >0$, a los términos que aparecen cero veces en un documento se les asigna una pequeña probabilidad.\n",
    "\n",
    "\n",
    "https://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"implementacions\"></a>\n",
    "\n",
    "#### Implementación del modelo\n",
    "\n",
    "\n",
    "[Volver al índice](#section_toc)\n",
    "\n",
    "\n",
    "Vamos a implementar el modelo, optimizando el hiperparámetro $\\alpha$ mediante una grid search cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 19 candidates, totalling 57 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.8463\n",
      "Best parameters:  {'alpha': 0.8500000000000001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  57 out of  57 | elapsed:    1.4s finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "skf=StratifiedKFold(n_splits=3,random_state=0,shuffle=True)\n",
    "\n",
    "param_grid={'alpha':np.arange(0.05,1,0.05)};\n",
    "grid = GridSearchCV(MultinomialNB(), param_grid, cv=skf,verbose=1);\n",
    "grid.fit(X_train, y_train);\n",
    "print(\"Best cross-validation score: {:.4f}\".format(grid.best_score_));\n",
    "print(\"Best parameters: \", grid.best_params_);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tf-idf\n",
    "\n",
    "Comparemos la performance con la obtenida usando la representacion tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 19 candidates, totalling 57 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.8634\n",
      "Best parameters:  {'alpha': 0.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  57 out of  57 | elapsed:    1.3s finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tfidf=TfidfTransformer();\n",
    "X_tfidf=tfidf.fit_transform(X_train);\n",
    "\n",
    "param_grid={'alpha':np.arange(0.05,1,0.05)};\n",
    "grid = GridSearchCV(MultinomialNB(), param_grid, cv=skf,verbose=1);\n",
    "grid.fit(X_tfidf, y_train);\n",
    "print(\"Best cross-validation score: {:.4f}\".format(grid.best_score_));\n",
    "print(\"Best parameters: \", grid.best_params_);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos una performance levemente mejor con Tf-idf. Veamos si podemos mejorar esta performace bajando la dimensionalidad de los datos. Empecemos por remover stopwords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 19 candidates, totalling 57 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.8655\n",
      "Best parameters:  {'alpha': 0.9000000000000001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  57 out of  57 | elapsed:    0.9s finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import stopwords \n",
    "stop_words=stopwords.words('english');\n",
    "\n",
    "vectorizer=TfidfVectorizer(stop_words=stop_words,strip_accents='unicode');\n",
    "\n",
    "X_tfidf_reduced=vectorizer.fit_transform(train);\n",
    "\n",
    "grid.fit(X_tfidf_reduced, y_train);\n",
    "print(\"Best cross-validation score: {:.4f}\".format(grid.best_score_));\n",
    "print(\"Best parameters: \", grid.best_params_);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos una leve mejoría, pero veamos cuánto hemos reducido efectivamente la dimensionalidad del dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Con stopwords: (18750, 66297)\n",
      "Sin stopwords: (18750, 66152)\n"
     ]
    }
   ],
   "source": [
    "print('Con stopwords:',X_train.shape)\n",
    "print('Sin stopwords:',X_tfidf_reduced.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos qué pasa si removemos términos que aparecen en muchos documentos: stopwords específicas de este corpus. El parámetro max_df remueve términos que aparecen en más que x documentos. Probemos removiendo palabras que aparecen en más de 100 documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionalidad: (18750, 63257)\n",
      "Fitting 3 folds for each of 19 candidates, totalling 57 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.7925\n",
      "Best parameters:  {'alpha': 0.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  57 out of  57 | elapsed:    0.8s finished\n"
     ]
    }
   ],
   "source": [
    "vectorizer=TfidfVectorizer(stop_words=stop_words,strip_accents='unicode',max_df=100);\n",
    "\n",
    "X_tfidf_reduced=vectorizer.fit_transform(train);\n",
    "\n",
    "print('Dimensionalidad:',X_tfidf_reduced.shape)\n",
    "\n",
    "grid.fit(X_tfidf_reduced, y_train);\n",
    "print(\"Best cross-validation score: {:.4f}\".format(grid.best_score_));\n",
    "print(\"Best parameters: \", grid.best_params_);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que el modelo empeoró. Si juegan un poco con este parámetro verán que luego de remover stopwords, no hay muchos términos que aparezcan en un porcentaje alto documentos del corpus. Esto se debe a que los comentarios sobre películas son bastante cortos y muchas palabras refieren específicamente al film."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigramas\n",
    "\n",
    "Probemos ahora incluyendo bigramas en el vocabulario. Es decir, considerar como términos a pares de palabras consecutivas en el texto. Tanto CountVectorizer como TfidfVectorizer tienen el parámetro ngram_range.\n",
    "\n",
    "De la documentación:\n",
    "\n",
    "<b> ngram_range</b>: tuple (min_n, max_n), default=(1, 1)\n",
    "The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used. For example an ngram_range of (1, 1) means only unigrams, (1, 2) means unigrams and bigrams, and (2, 2) means only bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de la matriz: (18750, 1464314) \n",
      "\n",
      "Fitting 3 folds for each of 19 candidates, totalling 57 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.8843\n",
      "Best parameters:  {'alpha': 0.15000000000000002}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  57 out of  57 | elapsed:    9.1s finished\n"
     ]
    }
   ],
   "source": [
    "# Incluimos bigramas\n",
    "\n",
    "vectorizer=TfidfVectorizer(stop_words=stop_words,strip_accents='unicode',ngram_range=(1,2));\n",
    "X_tfidf_bigrams=vectorizer.fit_transform(train);\n",
    "\n",
    "print('Dimensiones de la matriz:', X_tfidf_bigrams.shape,'\\n')\n",
    "\n",
    "\n",
    "grid.fit(X_tfidf_bigrams, y_train);\n",
    "print(\"Best cross-validation score: {:.4f}\".format(grid.best_score_));\n",
    "print(\"Best parameters: \", grid.best_params_);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.8936\n"
     ]
    }
   ],
   "source": [
    "# Performance de nuestro modelo en el test set:\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_test=vectorizer.transform(test);\n",
    "model=grid.best_estimator_;\n",
    "y_pred=model.predict(X_test);\n",
    "\n",
    "print('Test accuracy:',accuracy_score(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos qué predicción hace nuestro modelo sobre comentarios escritos por nosotros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comentario: ['this movie is terrible. I have never seen anything this bad in my life.'] \n",
      "\n",
      "Calificación: negativo  con proba: 0.9183280839667625\n",
      "\n",
      "Comentario: ['I highly recomend this film to anyone who likes documentaries. The director succeded in portraiyng the soul of the main character.'] \n",
      "\n",
      "Calificación: positivo  con proba: 0.8044476111771026\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "classes={0:'negativo',1:'positivo'};\n",
    "\n",
    "texto_prueba=['this movie is terrible. I have never seen anything this bad in my life.']\n",
    "texto_vec=vectorizer.transform(texto_prueba);\n",
    "clase=model.predict(texto_vec);\n",
    "\n",
    "print('Comentario:',texto_prueba,'\\n')\n",
    "print('Calificación:',classes[clase[0]],' con proba:',model.predict_proba(texto_vec).max())\n",
    "\n",
    "texto_prueba=['I highly recomend this film to anyone who likes documentaries. The director succeded in portraiyng the soul of the main character.']\n",
    "texto_vec=vectorizer.transform(texto_prueba);\n",
    "clase=model.predict(texto_vec);\n",
    "print('\\nComentario:',texto_prueba,'\\n')\n",
    "print('Calificación:',classes[clase[0]],' con proba:',model.predict_proba(texto_vec).max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comentario sobre la optimización del modelo\n",
    "\n",
    "Hemos estado optimizando el hiperparámetro $\\alpha$ y moviendo a mano otros hiperparámetros que intervienen en la vectorización: min_df, max_df, inclusión o no de stopwords.\n",
    "\n",
    "Para ser más precisos deberíamos armar un pipeline que incluya la verctorizacion y el clasificador para optimizar sobre todos los hiperparámetros a la vez. Obviamente esto tiene un mayor costo de cómputo, pero dejamos un código aquí abajo como referencia en donde se optimiza alpha y min_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:   32.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score: 0.8657066666666666\n",
      "best params: {'classifier__alpha': 1, 'vect__min_df': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "vectorizer=TfidfVectorizer(stop_words=stop_words,strip_accents='unicode');\n",
    "\n",
    "model=Pipeline([('vect',vectorizer),('classifier',MultinomialNB())])\n",
    "\n",
    "params={'classifier__alpha':[0.1,0.5,1],'vect__min_df':[1,3]};\n",
    "\n",
    "GS_CV=GridSearchCV(model,params,cv=skf,verbose=1,n_jobs=-1);\n",
    "\n",
    "GS_CV.fit(train, y_train);\n",
    "\n",
    "print('best score:',GS_CV.best_score_)\n",
    "print('best params:',GS_CV.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Más datasets\n",
    "\n",
    "Para practicar pueden buscar más datasets en los siguientes sitios:\n",
    "\n",
    "https://lionbridge.ai/datasets/14-best-text-classification-datasets-for-machine-learning/\n",
    "\n",
    "https://lionbridge.ai/datasets/15-free-sentiment-analysis-datasets-for-machine-learning/\n",
    "\n",
    "NLTK también provee datasets que pueden descargar desde la linea de comandos y manipularlos fácilmente con funciones de la librería. \n",
    "\n",
    "Referencia: https://www.nltk.org/book/ch02.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dhdsblend2021] *",
   "language": "python",
   "name": "conda-env-dhdsblend2021-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
