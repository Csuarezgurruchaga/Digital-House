{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/csuarezgurruchaga/Desktop/Digital-House/CLASE_47/dsad_2021/common\n",
      "default checking\n",
      "Running command `conda list`... ok\n",
      "jupyterlab=2.2.6 already installed\n",
      "pandas=1.1.5 already installed\n",
      "bokeh=2.2.3 already installed\n",
      "seaborn=0.11.0 already installed\n",
      "matplotlib=3.3.2 already installed\n",
      "ipywidgets=7.5.1 already installed\n",
      "pytest=6.2.1 already installed\n",
      "chardet=4.0.0 already installed\n",
      "psutil=5.7.2 already installed\n",
      "scipy=1.5.2 already installed\n",
      "statsmodels=0.12.1 already installed\n",
      "scikit-learn=0.23.2 already installed\n",
      "xlrd=2.0.1 already installed\n",
      "nltk=3.5 already installed\n",
      "unidecode=1.1.1 already installed\n",
      "pydotplus=2.0.2 already installed\n",
      "pandas-datareader=0.9.0 already installed\n",
      "flask=1.1.2 already installed\n"
     ]
    }
   ],
   "source": [
    "# initial setup\n",
    "%run \"../../../common/0_notebooks_base_setup.py\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../../../common/logo_DH.png' align='left' width=35%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint: Clasificacion de texto\n",
    "\n",
    "Trabajaremos con un dataset de noticias (en ingles) sobre diferentes temas: world, sports, business, sci/tech.\n",
    "\n",
    "La idea es vectorizar el corpus de noticias e implementar un clasificador para identificar a qué tema pertenecen las noticias.\n",
    "\n",
    "Trabajaremos con una versión reducida de un corpus que pueden descargar completo (~30Mb) del siguiente sitio:\n",
    "\n",
    "https://github.com/mhjabreel/CharCnn_Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 1\n",
    "\n",
    "Generamos el corpus:\n",
    "\n",
    "Importen los datos con pandas. Los datos se encuentran en '../Data/ag_news_reduced.csv'. \n",
    "\n",
    "El dataset contiene cuatro columnas: la primera tiene un entero que indica a qué clase pertenece la noticia. La segunda es el nombre de la clase, la tercera es el título y la cuarta es una descripción de la noticia.\n",
    "\n",
    "Como no tenemos las noticias enteras sino solo un resumen, conviene que generemos el corpus concatenando el título y la descripción en un sólo texto. \n",
    "\n",
    "Generen una columna en el dataframe que sea la concatenación del título y la descripción. No olviden agregar un espacio en blanco para no pegar la última palabra del título con la primera de la descrición.\n",
    "\n",
    "¿Cuántos artículos hay de cada clase?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>class_name</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Business</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
       "      <td>Reuters - Short-sellers, Wall Street's dwindli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Business</td>\n",
       "      <td>Google IPO faces Playboy slip-up</td>\n",
       "      <td>The bidding gets underway for Google's public ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Business</td>\n",
       "      <td>Letters</td>\n",
       "      <td>Target the abusers of legal weapons We can all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Business</td>\n",
       "      <td>Oldsmobile: The final parking lot</td>\n",
       "      <td>Why General Motors dropped the Oldsmobile. The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Sci/Tech</td>\n",
       "      <td>AOL to Sell Cheap PCs to Minorities and Senior...</td>\n",
       "      <td>Reuters - America Online on Thursday said it\\p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class class_name                                              title  \\\n",
       "0      3   Business  Wall St. Bears Claw Back Into the Black (Reuters)   \n",
       "1      3   Business                   Google IPO faces Playboy slip-up   \n",
       "2      3   Business                                            Letters   \n",
       "3      3   Business                  Oldsmobile: The final parking lot   \n",
       "4      4   Sci/Tech  AOL to Sell Cheap PCs to Minorities and Senior...   \n",
       "\n",
       "                                         description  \n",
       "0  Reuters - Short-sellers, Wall Street's dwindli...  \n",
       "1  The bidding gets underway for Google's public ...  \n",
       "2  Target the abusers of legal weapons We can all...  \n",
       "3  Why General Motors dropped the Oldsmobile. The...  \n",
       "4  Reuters - America Online on Thursday said it\\p...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data=pd.read_csv('../Data/ag_news_reduced.csv');\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['news']=data['title']+ ' '+data['description'];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['news'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sports      1539\n",
       "Sci/Tech    1520\n",
       "World       1486\n",
       "Business    1455\n",
       "Name: class_name, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['class_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 2\n",
    "\n",
    "¿Cuáles son las palabras más frecuentes dentro de cada clase?\n",
    "\n",
    "Ayuda: Pueden vectorizar el corpus (dividido por temas) usando CountVectorizer() y luego sumar las filas de la matriz para obtener el número total de veces que aparece cada término.\n",
    "\n",
    "Ayuda2: No olviden remover stopwords al vectorizar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Clase  World\n",
      "['39' 'said' 'iraq' 'ap' 'reuters' 'president' 'us' 'two' 'afp' 'minister'\n",
      " 'people' 'new' 'quot' 'killed' 'government' 'baghdad' 'bush' 'iraqi'\n",
      " 'palestinian' 'prime']\n",
      "\n",
      " Clase  Sports\n",
      "['39' 'ap' 'new' 'first' 'game' 'team' 'two' 'season' 'win' 'one' 'night'\n",
      " 'cup' 'world' 'year' 'league' 'sunday' 'last' 'victory' 'quot' 'time']\n",
      "\n",
      " Clase  Business\n",
      "['39' 'reuters' 'said' 'oil' 'new' 'us' 'gt' 'lt' 'stocks' 'inc' 'company'\n",
      " 'prices' 'fullquote' 'york' 'corp' 'monday' 'percent' 'sales' 'year'\n",
      " 'tuesday']\n",
      "\n",
      " Clase  Sci/Tech\n",
      "['39' 'new' 'microsoft' 'software' 'said' 'ap' 'lt' 'gt' 'reuters'\n",
      " 'internet' 'company' 'space' 'music' 'search' 'quot' 'inc' 'security'\n",
      " 'world' 'technology' 'com']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "stop_words=stopwords.words('english');\n",
    "\n",
    "vectorizer=CountVectorizer(stop_words=stop_words);\n",
    "\n",
    "clases=['World','Sports','Business','Sci/Tech'];\n",
    "\n",
    "for clase in range(1,5):\n",
    "    X=vectorizer.fit_transform(data[data['class']==clase]['news']);\n",
    "    counts=X.sum(axis=0);\n",
    "    counts=np.array(counts);\n",
    "    \n",
    "    indices=np.argsort(counts);\n",
    "    valores=np.sort(counts);\n",
    "    indices=indices[0][::-1];\n",
    "    valores=valores[0][::-1];\n",
    "    terms=np.array(vectorizer.get_feature_names());\n",
    "\n",
    "    print('\\n Clase ',clases[clase-1])\n",
    "    print(terms[indices[:20]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 3\n",
    "\n",
    "Vectorizar el corpus usando TfidfVectorizer y entrenar un clasificador de tipo Multinomial Naive Bayes (MultinomialNB). Para empezar, hagan un simple train_test_split de los datos y vean la performance (accuracy) en el set de validacion.\n",
    "\n",
    "Luego vean si pueden mejorar esa performance optimizando el parámetro alpha del modelo haciendo una Gridsearch cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8686666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV,StratifiedKFold,train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "train,test=train_test_split(data,stratify=data['class'],random_state=3);\n",
    "\n",
    "vectorizer=TfidfVectorizer();\n",
    "X_train=vectorizer.fit_transform(train['news']);\n",
    "y_train=train['class'];\n",
    "\n",
    "X_test=vectorizer.transform(test['news']);\n",
    "y_test=test['class'];\n",
    "\n",
    "nbc=MultinomialNB();\n",
    "\n",
    "nbc.fit(X_train,y_train);\n",
    "y_pred=nbc.predict(X_test);\n",
    "\n",
    "print('Accuracy:',accuracy_score(y_test,y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score: 0.8835000000000001\n",
      "best params: {'alpha': 0.16000000000000003}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  60 out of  60 | elapsed:    2.2s finished\n",
      "/Users/csuarezgurruchaga/opt/anaconda3/envs/dhdsblend2021/lib/python3.8/site-packages/sklearn/model_selection/_search.py:847: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Optimizamos en alpha\n",
    "\n",
    "vectorizer=TfidfVectorizer();\n",
    "\n",
    "X=vectorizer.fit_transform(data['news']);\n",
    "y=data['class'];\n",
    "\n",
    "skf=StratifiedKFold(n_splits=3,random_state=3,shuffle=True);\n",
    "\n",
    "params={'alpha':np.arange(0.01,1,0.05)};\n",
    "GS_CV=GridSearchCV(MultinomialNB(),params,cv=skf,verbose=1,n_jobs=3,iid=False);\n",
    "GS_CV.fit(X,y);\n",
    "print('best score:',GS_CV.best_score_)\n",
    "print('best params:',GS_CV.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 19590)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 4\n",
    "\n",
    "Repetir lo anterior removiendo stopwords al vectorizar.\n",
    "\n",
    "¿Cambia la performance?  ¿Cuánto se redujo la dimensionalidad?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score: 0.8856666666666667\n",
      "best params: {'alpha': 0.41000000000000003}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  60 out of  60 | elapsed:    0.2s finished\n",
      "/Users/csuarezgurruchaga/opt/anaconda3/envs/dhdsblend2021/lib/python3.8/site-packages/sklearn/model_selection/_search.py:847: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Include Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=stopwords.words('english');\n",
    "stop_words.append('39');\n",
    "stop_words.append('reuters');\n",
    "\n",
    "vectorizer=TfidfVectorizer(stop_words=stop_words);\n",
    "\n",
    "X_train=vectorizer.fit_transform(data['news']);\n",
    "y_train=data['class'];\n",
    "\n",
    "skf=StratifiedKFold(n_splits=3,random_state=0,shuffle=True);\n",
    "params={'alpha':np.arange(0.01,1,0.05)};\n",
    "GS_CV=GridSearchCV(MultinomialNB(),params,cv=skf,verbose=1,n_jobs=3,iid=False);\n",
    "GS_CV.fit(X_train,y_train);\n",
    "print('best score:',GS_CV.best_score_)\n",
    "print('best params:',GS_CV.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 19453)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 5\n",
    "\n",
    "Utilizar los parámetros min_df y max_df para remover términos que aparecen en muy pocos documentos o que aparecen en demasiados (stopwords específicas del corpus).\n",
    "\n",
    "Reentrenar el modelo y ver si cambia la performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 10430)\n",
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score: 0.8858333333333333\n",
      "best params: {'alpha': 0.3500000000000001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  54 out of  54 | elapsed:    0.2s finished\n",
      "/Users/csuarezgurruchaga/opt/anaconda3/envs/dhdsblend2021/lib/python3.8/site-packages/sklearn/model_selection/_search.py:847: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# min_df\n",
    "\n",
    "vectorizer=TfidfVectorizer(stop_words=stop_words,min_df=2,max_df=0.8);\n",
    "X_train=vectorizer.fit_transform(data['news']);\n",
    "y_train=data['class'];\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "skf=StratifiedKFold(n_splits=3,random_state=0,shuffle=True);\n",
    "params={'alpha':np.arange(0.1,1,0.05)};\n",
    "GS_CV=GridSearchCV(MultinomialNB(),params,cv=skf,verbose=1,n_jobs=3,iid=False);\n",
    "GS_CV.fit(X_train,y_train);\n",
    "print('best score:',GS_CV.best_score_)\n",
    "print('best params:',GS_CV.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 6\n",
    "\n",
    "Repetir el análisis incluyendo bigramas en la vectorizacion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 21541)\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  30 out of  30 | elapsed:    0.1s finished\n",
      "/Users/csuarezgurruchaga/opt/anaconda3/envs/dhdsblend2021/lib/python3.8/site-packages/sklearn/model_selection/_search.py:847: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score: 0.8886666666666666\n",
      "best params: {'alpha': 0.09000000000000001}\n"
     ]
    }
   ],
   "source": [
    "# n-grams\n",
    "\n",
    "vectorizer=TfidfVectorizer(stop_words=stop_words,min_df=2,ngram_range=(1,2));\n",
    "\n",
    "X_train=vectorizer.fit_transform(data['news']);\n",
    "y_train=data['class'];\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "skf=StratifiedKFold(n_splits=3,random_state=0,shuffle=True);\n",
    "params={'alpha':np.arange(0.05,0.15,0.01)};\n",
    "GS_CV=GridSearchCV(MultinomialNB(),params,cv=skf,verbose=1,n_jobs=3,iid=False);\n",
    "GS_CV.fit(X_train,y_train);\n",
    "print('best score:',GS_CV.best_score_)\n",
    "print('best params:',GS_CV.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dhdsblend2021] *",
   "language": "python",
   "name": "conda-env-dhdsblend2021-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
